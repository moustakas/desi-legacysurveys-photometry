#!/usr/bin/env python

"""Match data release redshift catalogs against the original photometric
(target) catalogs.

Set up the software dependencies but make sure we're using the desispec/0.53.2 tag, which is not yet part of a major release:
  source /global/common/software/desi/desi_environment.sh 22.2
  module unload desispec
  export PYTHONPATH=$HOME/code/desihub/desispec/py:${PYTHONPATH}

Sequence of commands for a given release (here, fuji)
1. Gather targeting photometry from the individual redshift catalogs.

   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir $DESI_ROOT/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --specprod fuji --mp 32 --targetphot

   Output files:
    targetphot-fuji.fits
    targetphot-zcat-fuji.fits
    targetphot-missing-fuji.fits

2. Gather Tractor photometry.

   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir $DESI_ROOT/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --specprod fuji --mp 32 --tractorphot

   Output files:
     tractorphot-nside4-hp???-fuji.fits

3. Gather targeting and Tractor photometry for *potential* targets (going back to the original fiberassign files).

   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir $DESI_ROOT/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --specprod fuji --mp 32 --targetphot --potential
   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir $DESI_ROOT/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --specprod fuji --mp 32 --tractorphot --potential

   Output files:
     targetphot-potential-fuji.fits
     targetphot-potential-zcat-fuji.fits
     targetphot-potential-missing-fuji.fits
     tractorphot-potential-nside4-hp???-fuji.fits

"""
import os, sys, argparse, time, pdb
from glob import glob
import numpy as np
import fitsio
import multiprocessing
import healpy as hp

import astropy
from astropy.io import fits
from astropy.table import Table, vstack, join

from desispec.io import specprod_root
from desimodel.footprint import radec2pix
from desitarget.targets import decode_targetid
from desiutil.brick import brickname
from desispec.io.util import fitsheader
from desiutil.depend import add_dependencies

from desiutil.log import get_logger, DEBUG
log = get_logger()

desi_root = os.environ.get('DESI_ROOT')
fiberassign_dir = os.path.join(desi_root, 'target', 'fiberassign', 'tiles', 'trunk')

def _read_one_zcat(args):
    """Multiprocessing wrapper."""
    return read_one_zcat(*args)

def read_one_zcat(catfile):
    """Read a single redshift catalog, e.g., ztile-sv1-dark-cumulative.fits.

    Args:
        catfile (str): full path to a given redshift catalog

    Returns an astropy.table.Table with the following columns needed to do the
    downstream matching and to enable QA: TARGETID, TILEID, TARGET_RA,
    TARGET_DEC, PETAL_LOC.

    """
    log.info('Reading {}'.format(catfile))
    hdr = fitsio.read_header(catfile, ext='ZCATALOG')
    survey, program = hdr['SURVEY'], hdr['PROGRAM']
    cat = Table(fitsio.read(catfile, ext='ZCATALOG', columns=[
        'TARGETID', 'TILEID', 'TARGET_RA', 'TARGET_DEC', 'PETAL_LOC']))
    # Remove duplicate observations; e.g., in survey=sv1, program=other,
    # tileid=80870 there are 8384 observations, but only 4192 of those are
    # unique targetids.
    indx = []
    for tileid in np.unique(cat['TILEID']):
        I = np.where(tileid == cat['TILEID'])[0]
        _, uindx = np.unique(cat['TARGETID'][I], return_index=True)
        indx.append(I[uindx])
    indx = np.hstack(indx)
    cat = cat[indx]
    cat['SURVEY'] = survey
    cat['PROGRAM'] = program
    return cat

def _read_one_potential_targets(args):
    """Multiprocessing wrapper."""
    return read_one_potential_targets(*args)

def read_one_potential_targets(tileid, survey, program):
    """Read the potential targets in a given fiberassign tile.

    Args:
        tileid (int): tile ID number

    Returns an astropy.table.Table with the following columns needed downstream:
    TARGETID, RA, DEC.

    """
    stileid = '{:06d}'.format(tileid)
    fiberfile = os.path.join(fiberassign_dir, stileid[:3], 'fiberassign-{}.fits.gz'.format(stileid))
    #log.info('Reading {}'.format(fiberfile))

    # old code that used POTENTIAL_ASSIGNMENTS
    #targetid = fitsio.read(fiberfile, ext='POTENTIAL_ASSIGNMENTS', columns='TARGETID')
    #targetid = np.unique(targetid)
    ## remove skies
    ##_, _, _, _, sky, _ = decode_targetid(targetid)
    #objid, brickid, release, mock, sky, gaia = decode_targetid(targetid)
    #keep = (sky == 0) * (targetid > 0)
    #out = Table()
    #if np.sum(keep) == 0:
    #    return out
    #out['TARGETID'] = targetid[keep]
    #out['TILEID'] = tileid
    #
    #targets = Table(fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC']))
    #out = join(out, targets, keys='TARGETID', join_type='left')

    out = Table(fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC']))

    # remove skies
    _, _, _, _, sky, _ = decode_targetid(out['TARGETID'])
    keep = (sky == 0) * (out['TARGETID'] > 0)
    out = out[keep]

    out['TILEID'] = tileid
    out['SURVEY'] = survey
    out['PROGRAM'] = program

    # Old code to populate the brick columns. However, this procedure is not
    # correct for secondary targets, so don't do it.
    #from desitarget.io import release_to_photsys
    #bricks = Table(fitsio.read('/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/survey-bricks.fits.gz'))
    #out['BRICKID'] = brickid[keep]
    #out['OBJID'] = objid[keep]
    #out['RELEASE'] = release[keep]
    #out['BRICKNAME'] = np.zeros(len(out), dtype=bricks['BRICKNAME'].dtype)
    #out['PHOTSYS'] = np.zeros(len(out), dtype='U1')
    #idr9 = np.where((out['RELEASE'] > 9000) * (out['RELEASE'] < 9050))[0]
    #if len(idr9) > 0:
    #    photsys = release_to_photsys(out['RELEASE'][idr9])
    #    out['PHOTSYS'][idr9] = photsys
    #    for bid in set(out['BRICKID'][idr9]):
    #        J = bid == bricks['BRICKID']
    #        I = np.where(bid == out['BRICKID'][idr9])[0]
    #        out['BRICKNAME'][idr9[I]] = bricks['BRICKNAME'][J]
    return out

def _cache_one_catalog(args):
    """Multiprocessing wrapper."""
    return cache_one_catalog(*args)

def cache_one_catalog(cachefile):
    """Cache a single (secondary) targeting catalog, e.g.,
    $DESI_ROOT/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits

    Args:
        cachefile (str): full path to a targeting catalog.

    Returns a dictionary mapping the input filename to either an np.ndarray of
    TARGETID values (for a non-TOO catalog) and a full astropy.table.Table for
    an input TOO catalog name.

    """
    log.info('Caching {}'.format(cachefile))
    if '.ecsv' in cachefile:
        cat = Table.read(cachefile, guess=False, format='ascii.ecsv')
        key = 'TOO'
    else:
        cat = fitsio.read(cachefile, columns='TARGETID') # just targetid
        key = cachefile
    return {key: cat}

def _tractorphot_one(args):
    """Multiprocessing wrapper."""
    return tractorphot_one(*args)

def tractorphot_one(cat, racolumn='TARGET_RA', deccolumn='TARGET_DEC'):
    """Simple wrapper on desispec.io.photo.gather_tractorphot."""
    from desispec.io.photo import gather_tractorphot
    tractorphot = gather_tractorphot(cat, racolumn=racolumn, deccolumn=deccolumn)
    return tractorphot

def _targetphot_one(args):
    """Multiprocessing wrapper."""
    return targetphot_one(*args)

def targetphot_one(input_cat, photocache=None, racolumn='TARGET_RA', deccolumn='TARGET_DEC'):
    """Simple wrapper on desispec.io.photo.gather_targetphot."""
    from desispec.io.photo import gather_targetphot
    print('Working on tile {}'.format(input_cat['TILEID'][0]))

    # should be unique!
    assert(len(input_cat) == len(np.unique(input_cat['TARGETID'])))

    targetphot = gather_targetphot(input_cat, photocache=photocache,
                                   racolumn=racolumn, deccolumn=deccolumn)

    # Can have the same targetid across different surveys and even within a
    # survey, across different tiles. So we need these columns.
    # See https://github.com/moustakas/desi-photometry/issues/3
    targetphot['SURVEY'] = input_cat['SURVEY']
    targetphot['PROGRAM'] = input_cat['PROGRAM']
    targetphot['TILEID'] = input_cat['TILEID']

    return targetphot

def _get_secondary_targetdirs():
    """Scrape all fiberassign headers to find which secondary catalogs are ever used.

    The list of files returned by this script (after being run once) can then be
    cached using the photocache dictionary.

    """
    fiberfiles = np.hstack(glob(os.path.join(fiberassign_dir, '???', 'fiberassign-*.fits*')))
    targetdirs = []
    for ii, fiberfile in enumerate(fiberfiles):
        if ii % 500 == 0:
            log.info('Working on fiberfile {}/{}'.format(ii, len(fiberfiles)))
        fahdr = fits.getheader(fiberfile, ext=0)
        if 'SCND' in fahdr:
            if fahdr['SCND'].strip() != '-':
                targetdirs += [fahdr['SCND']]
    targetdirs = np.unique(np.hstack(targetdirs))
    final_targetdirs = []
    for ii, targetdir in enumerate(targetdirs):
        # can be a KPNO directory!
        if 'DESIROOT' in targetdir:
            targetdir = os.path.join(desi_root, targetdir.replace('DESIROOT/', ''))
        if targetdir[:6] == '/data/':
            targetdir = os.path.join(desi_root, targetdir.replace('/data/', ''))
        if os.path.isdir(targetdir):
            targetdir = glob(os.path.join(targetdir, '*.fits'))
        for targetdir1 in np.atleast_1d(targetdir):
            if os.path.isfile(targetdir1):
                log.info('Found secondary targets catalog {}'.format(targetdir1))
                final_targetdirs.append(targetdir1)
            else:
                log.warning('Targets directory {} not found.'.format(targetdir))
    final_targetdirs = np.unique(final_targetdirs)
    log.debug(final_targetdirs)

    return final_targetdirs

def _build_secondarycache(mp=1):
    """To speed things up, read and cache all the large secondary target catalogs
    and the TOO files *once*. Now, not all these catalogs are actually used but
    we don't know which ones yet until we read all the fiberassign headers.

    """
    # Figure out which secondary target catalogs actually were used to design
    # tiles but only do this one time.
    #cachefiles = _get_secondary_targetdirs()

    cachefiles = np.hstack([[
        #os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary.fits',
        #os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary-dr9photometry.fits',
        #os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        #os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        #os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.57.0/targets/sv3/secondary/bright/sv3targets-bright-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/0.57.0/targets/sv3/secondary/dark/sv3targets-dark-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/1.0.0/targets/main/secondary/bright/targets-bright-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/1.0.0/targets/main/secondary/dark/targets-dark-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/1.1.1/targets/main/secondary/bright/targets-bright-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/catalogs/dr9/1.1.1/targets/main/secondary/dark/targets-dark-secondary.fits',
        os.environ.get('DESI_ROOT')+'/target/secondary/sv1/dedicated/0.49.0/DC3R2_GAMA_priorities.fits'],
        glob(os.environ.get('DESI_ROOT')+'/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')])

    mpargs = []
    for cachefile in np.unique(cachefiles):
        #if cachefile.replace('.fits', '-dr9photometry.fits') in cachefiles and '.ecsv' not in cachefile: # just read the photometry
        #    continue
        mpargs.append([cachefile])
    if mp > 1:
        with multiprocessing.Pool(mp) as P:
            photocache1 = P.map(_cache_one_catalog, mpargs)
    else:
        photocache1 = [cache_one_catalog(mparg[0]) for mparg in mpargs]

    photocache = {}
    for _photocache1 in photocache1:
        for key in _photocache1.keys():
            if key in photocache.keys():
                photocache[key] = vstack((_photocache1[key], photocache[key]), metadata_conflicts='silent') # stack the TOO catalogs
        photocache.update(_photocache1)

    return photocache


def _most_recent_targeting():
    """Placeholder
    """
    return


def main():
    """Entry-point for command-line scripts.

    Returns
    -------
    :class:`int`
        An integer suitable for passing to :func:`sys.exit`.
    """
    p = argparse.ArgumentParser()
    p.add_argument('--reduxdir', type=str, help='spectro redux base dir overrides $DESI_SPECTRO_REDUX/$SPECPROD')
    p.add_argument('-o', '--outdir', type=str, required=True, help='output directory file')
    p.add_argument('--specprod', type=str, required=True, help='output file prefix')
    p.add_argument('--mp', type=int, default=1, help='number of multiprocessing cores')
    p.add_argument('--filenside', type=int, default=4, help='healpix nside for tractorphot catalogs.')
    p.add_argument('--no-secondarycache', action='store_true', help='Do not cache the secondary target catalogs.')
    p.add_argument('--ntest', type=int, default=None, help='Test on a randomly selected sample of NTEST objects.')
    p.add_argument('--potential', action='store_true', help='Gather photometry for the potential targets.')
    p.add_argument('--targetphot', action='store_true', help='Build the photo-targets catalogs.')
    p.add_argument('--validate-targetphot', action='store_true', help='Validate the targetphot catalogs.')
    p.add_argument('--tractorphot', action='store_true', help='Build the photo-tractor catalogs.')
    p.add_argument('--validate-tractorphot', action='store_true', help='Validate the photo-tractor catalogs.')
    p.add_argument('--overwrite', action='store_true', help='Overwrite existing photo-targets files.')
    p.add_argument('--latest', action='store_true', help='Use the most recent targeting instead of the targeting file specified in the fiberassign header.')

    args = p.parse_args()
    log = get_logger()

    if args.reduxdir is None:
        args.reduxdir = specprod_root()

    if not os.path.isdir(args.outdir):
        os.makedirs(args.outdir, exist_ok=True)
    for subdir in ['observed-targets', 'potential-targets']:
        if not os.path.isdir(os.path.join(args.outdir, subdir)):
            os.makedirs(os.path.join(args.outdir, subdir), exist_ok=True)
        if not os.path.isdir(os.path.join(args.outdir, subdir, 'tractorphot')):
            os.makedirs(os.path.join(args.outdir, subdir, 'tractorphot'), exist_ok=True)
    if not os.path.isdir(os.path.join(args.outdir, 'ancillary')):
        os.makedirs(os.path.join(args.outdir, 'ancillary'), exist_ok=True)

    if args.ntest is not None:
        rand = np.random.RandomState(seed=1)
    else:
        rand = None

    # Build the targetphot catalog.
    if args.targetphot:
        if args.potential:
            any_outfile = glob(os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-*-{}.fits'.format(args.specprod)))
        else:
            any_outfile = glob(os.path.join(args.outdir, 'observed-targets', 'targetphot-*-{}.fits'.format(args.specprod)))

        if len(any_outfile) > 0 and not args.overwrite:
            log.info('One or more targetphot output files exist; use --overwrite')
            return

        # Optionally cache the targetids of all the secondary target
        # catalogs. This cache is used in desispec.io.photo.gather_targetphot.
        if args.no_secondarycache:
            photocache = {}
        else:
            photocache = _build_secondarycache(mp=args.mp)

        if args.potential:
            # Assume that the nominal/parent zcat catalog has been previously generated.
            parent_zcatfile = os.path.join(args.outdir, 'ancillary', 'targetphot-zcat-{}.fits'.format(args.specprod))
            if not os.path.isfile(parent_zcatfile):
                errmsg = 'Targeting photometric catalog {} missing'.format(parent_zcatfile)
                log.critical(errmsg)
                raise IOError(errmsg)

            log.info('Reading {}'.format(parent_zcatfile))
            parent_zcat = Table(fitsio.read(parent_zcatfile))

            if rand is not None:
                #parent_zcat = parent_zcat[parent_zcat['TILEID'] == 81108]
                J = rand.choice(len(parent_zcat), size=args.ntest, replace=False)
                parent_zcat = parent_zcat[J]
            log.info('Read {:,} objects from {}'.format(len(parent_zcat), parent_zcatfile))

            #print('RESTRICTING TO SURVEY sv2!')
            #uindx = parent_zcat['SURVEY'] == 'sv2'
            #print('RESTRICTING TO TILE 81056!! Note uindx[0]!')
            #uindx = parent_zcat['TILEID'] == 81056
            #parent_zcat = parent_zcat[uindx]

            #tileids = np.unique(parent_zcat['TILEID'])
            _, uindx = np.unique(parent_zcat['TILEID'], return_index=True)
            tileids = parent_zcat['TILEID'][uindx]
            surveys = parent_zcat['SURVEY'][uindx]
            programs = parent_zcat['PROGRAM'][uindx]

            # Multiprocess over tiles
            #log.info('Dividing the sample into {} unique tiles.'.format(len(tileids)))
            mpargs = [[tileid, survey, program] for tileid, survey, program in zip(tileids, surveys, programs)]

            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    zcat = P.map(_read_one_potential_targets, mpargs)
            else:
                zcat = [read_one_potential_targets(*mparg) for mparg in mpargs]
            zcat = vstack(zcat)

            # Do not select unique TARGETIDs because the same target can appear
            # in different surveys. We handle uniqueness below.

            ## Select unique TARGETIDs.
            #_, uindx = np.unique(zcat['TARGETID'], return_index=True)
            #zcat = zcat[uindx]
            #zcat = zcat[np.argsort(zcat['TARGETID'])]

            log.info('Found {:,} objects from {:,} unique tiles'.format(
                len(zcat), len(set(zcat['TILEID']))))

            RACOLUMN, DECCOLUMN = 'RA', 'DEC'
        else:
            # Read all the zpix catalogs in parallel.
            zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
            #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-sv1-other-cumulative.fits'))
            mpargs = []
            for zcatfile in zcatfiles:
                mpargs.append([zcatfile])
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    zcat = P.map(_read_one_zcat, mpargs)
            else:
                zcat = [read_one_zcat(mparg[0]) for mparg in mpargs]

            # Stack and then remove negative targetids and sky fibers.
            zcat = vstack(zcat)

            _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
            keep = (sky == 0) * (zcat['TARGETID'] > 0)
            if np.sum(keep) > 0:
                log.info('Keeping {}/{} objects after removing sky targets and stuck positioners'.format(
                    np.sum(keep), len(zcat)))
                zcat = zcat[keep]

            log.info('Read {:,} objects from {} redshift catalogs'.format(len(zcat), len(zcatfiles)))

            RACOLUMN, DECCOLUMN = 'TARGET_RA', 'TARGET_DEC'

            #zcat[(zcat['SURVEY'] == 'sv1') * (zcat['PROGRAM'] == 'other') * (zcat['TILEID'] == 80870)]

        # Unfortunately, the same TARGETID can appear in different surveys
        # (e.g., sv1 vs sv3) with different targeting information. E.g., a
        # target can be a secondary target in one survey but a primary
        # (bright or dark) target in a different survey.

        # Here's one example:
        #      TARGETID     PETAL_LOC     TARGET_RA          TARGET_DEC     TILEID SURVEY PROGRAM
        #       int64         int16        float64            float64       int32   str7    str6
        # ----------------- --------- ------------------ ------------------ ------ ------ -------
        # 39627764155813825         0 213.92212168463337 -1.104593197988166  80974    sv1   other
        # 39627764155813825         8 213.92212168463337 -1.104593197988166    531    sv3  bright
        # 39627764155813825         8 213.92212168463337 -1.104593197988166    521    sv3    dark

        #key = np.array(['{}-{}'.format(survey, targetid) for survey, targetid in
        #       zip(zcat['SURVEY'], zcat['TARGETID'])])
        #_, uindx = np.unique(key, return_index=True)
        #
        ## https://stackoverflow.com/questions/30003068/how-to-get-a-list-of-all-indices-of-repeated-elements-in-a-numpy-array
        #idx_sort = np.argsort(key, kind='mergesort')
        #key_sorted = key[idx_sort]
        #vals, idx_start, count = np.unique(key_sorted, return_counts=True, return_index=True)
        #res = np.split(idx_sort, idx_start[1:])

        # Divide the analysis by survey.
        allout, allzcat = [], []
        allsurveys = zcat['SURVEY']
        #for survey in ['sv1']:
        for survey in sorted(set(allsurveys)):
            log.info('Working on survey {}'.format(survey))

            S = np.where(survey == allsurveys)[0]
            zcat_survey = zcat[S]

            if rand is not None:
                J = rand.choice(len(zcat_survey), size=args.ntest, replace=False)
                zcat_survey = zcat_survey[J]

            # important! sort by tile so we can stack without sorting, below
            zcat_survey = zcat_survey[np.argsort(zcat_survey['TILEID'])]

            # Do not select unique TARGETIDs!
            # See https://github.com/moustakas/desi-photometry/issues/3

            ## Select unique TARGETIDs.
            #_, uindx = np.unique(zcat_survey['TARGETID'], return_index=True)
            #zcat_survey = zcat_survey[uindx]
            #zcat_survey = zcat_survey[np.argsort(zcat_survey['TARGETID'])]

            log.info('Found {:,} TARGETIDs and {:,} unique tiles from survey: {}'.format(
                len(zcat_survey), len(set(zcat_survey['TILEID'])), survey))

            # Multiprocess over tiles. Note: originally we multiprocessed the
            # non-potential targets over nside=8 healpixels which is how the
            # non-secondary targeting catalogs are organized on-disk. However,
            # this is problematic because you can have the same target appear in
            # both SV1 and SV3 without enough discriminating information in
            # desispec.io.photo.gather_targetphot to determine which object was
            # actually targeted. Multiprocessing over tiles and surveys solves
            # this issue.

            # count the number of targets in each tile
            #npertile = [np.sum(zcat_survey['TILEID'] == tileid) for tileid in np.unique(zcat_survey['TILEID'])]

            #log.info('Dividing the sample into {} unique tiles.'.format(len(set(zcat_survey['TILEID']))))
            mpargs = [[zcat_survey[zcat_survey['TILEID'] == tileid], photocache, RACOLUMN, DECCOLUMN] for tileid in np.unique(zcat_survey['TILEID'])]

            # In fuji/sv1, multiprocessing is suuuuuuuperslow, so skip it here.

            t0 = time.time()
            if survey != 'sv1' and args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    out = P.map(_targetphot_one, mpargs)
            else:
                out = [targetphot_one(*mparg) for mparg in mpargs]
            log.info('Total time for gather_targetphot = {:.3f} min'.format((time.time()-t0)/60))

            # stack, sort, and write out
            if len(out) > 0:
                out = vstack(out)
                try:
                    assert(np.all(out['TARGETID'] == zcat_survey['TARGETID']))
                except:
                    pdb.set_trace()

                if args.potential:
                    outfile = os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-{}-{}.fits'.format(survey, args.specprod))
                else:
                    outfile = os.path.join(args.outdir, 'observed-targets', 'targetphot-{}-{}.fits'.format(survey, args.specprod))

                add_dependencies(out.meta)
                out.meta['SURVEY'] = (survey, 'survey name')
                out.meta['EXTNAME'] = 'TARGETPHOT'
                log.info('Writing {:,} objects to {}'.format(len(out), outfile))
                out.write(outfile, overwrite=True)

                out['SURVEY'] = survey
                allout.append(out)
                del out

                # Also write out the redshift catalog (with a minimal number
                # of columns) used to generate the targetphot file. Note
                # that this intermediate catalog is needed / used to do
                # positional Tractor photometry gathering, below.
                if args.potential:
                    outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-zcat-{}-{}.fits'.format(survey, args.specprod))
                else:
                    outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-zcat-{}-{}.fits'.format(survey, args.specprod))

                add_dependencies(zcat_survey.meta)
                zcat_survey.meta['SURVEY'] = (survey, 'survey name')
                zcat_survey.meta['EXTNAME'] = 'ZCATALOG'
                log.info('Writing {:,} objects to {}'.format(len(zcat_survey), outfile_zcat))
                zcat_survey.write(outfile_zcat, overwrite=True)
                allzcat.append(zcat_survey)

        if len(allout) > 0:
            if args.potential:
                outfile = os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-{}.fits'.format(args.specprod))
            else:
                outfile = os.path.join(args.outdir, 'observed-targets', 'targetphot-{}.fits'.format(args.specprod))

            allout = vstack(allout, metadata_conflicts='silent')
            allout.meta.pop('SURVEY')
            #allout.meta['EXTNAME'] = 'TARGETPHOT'
            log.info('Writing {:,} objects to {}'.format(len(allout), outfile))
            allout.write(outfile, overwrite=True)

            if args.potential:
                outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-zcat-{}.fits'.format(args.specprod))
            else:
                outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-zcat-{}.fits'.format(args.specprod))

            allzcat = vstack(allzcat, metadata_conflicts='silent')

            try:
                assert(np.all(allout['TARGETID'] == allzcat['TARGETID']))
            except:
                pdb.set_trace()

            allzcat.meta.pop('SURVEY')
            log.info('Writing {:,} objects to {}'.format(len(allzcat), outfile_zcat))
            allzcat.write(outfile_zcat, overwrite=True)

        del photocache, zcat, allout, allzcat

    # Build the tractorphot catalogs.
    if args.tractorphot:
        if args.potential:
            any_outfile = glob(os.path.join(args.outdir, 'potential-targets', 'tractorphot', 'tractorphot-potential-nside{}-hp*-{}.fits'.format(
                args.filenside, args.specprod)))
        else:
            any_outfile = glob(os.path.join(args.outdir, 'observed-targets', 'tractorphot', 'tractorphot-nside{}-hp*-{}.fits'.format(
                args.filenside, args.specprod)))
        if len(any_outfile) > 0 and not args.overwrite:
            log.info('One or more tractorphot output files exist; use --overwrite')
            return

        if args.potential:
            zcatfile = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-zcat-{}.fits'.format(args.specprod))
            targetfile = os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-{}.fits'.format(args.specprod))
            racolumn, deccolumn = 'RA', 'DEC'
        else:
            zcatfile = os.path.join(args.outdir, 'ancillary', 'targetphot-zcat-{}.fits'.format(args.specprod))
            targetfile = os.path.join(args.outdir, 'observed-targets', 'targetphot-{}.fits'.format(args.specprod))
            racolumn, deccolumn = 'TARGET_RA', 'TARGET_DEC'

        if not os.path.isfile(targetfile):
            errmsg = 'Targeting photometric catalog {} missing'.format(targetfile)
            log.critical(errmsg)
            raise IOError(errmsg)

        if not os.path.isfile(zcatfile):
            errmsg = 'Targeting redshift catalog {} missing'.format(zcatfile)
            log.critical(errmsg)
            raise IOError(errmsg)

        if rand is not None:
            N = fitsio.FITS(targetfile)[1].get_nrows()
            J = rand.choice(N, size=args.ntest, replace=False)
            log.info('Reading {}'.format(targetfile))
            cat = Table(fitsio.read(targetfile, rows=J))
            log.info('Reading {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile, rows=J))
        else:
            log.info('Reading {}'.format(targetfile))
            cat = Table(fitsio.read(targetfile))
            log.info('Reading {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile))
        try:
            assert(np.all(cat['TARGETID'] == zcat['TARGETID']))
        except:
            pdb.set_trace()

        # TARGETIDs can repeat in and between surveys, so select just unique
        # targets here.
        _, uindx = np.unique(cat['TARGETID'], return_index=True)
        cat = cat[uindx]
        zcat = zcat[uindx]
        log.info('Selecting {:,} unique targets.'.format(len(cat)))

        # Create a mini-cat that we'll use for the searching.
        minicat = cat['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID']#, 'RA', 'DEC']
        minicat[racolumn] = zcat[racolumn]
        minicat[deccolumn] = zcat[deccolumn]

        # Some secondary programs (e.g., 39632961435338613, 39632966921487347)
        # have BRICKNAME!='' & BRICKID!=0, but BRICK_OBJID==0. Unpack those here
        # using decode_targetid.
        fix = np.where((minicat['BRICKNAME'] != '') * (minicat['BRICK_OBJID'] == 0))[0]
        if len(fix) > 0:
            log.info('Inferring BRICK_OBJID for {} objects using decode_targetid'.format(len(fix)))
            fix_objid, fix_brickid, _, _, _, _ = decode_targetid(minicat['TARGETID'][fix])
            assert(np.all(fix_brickid == minicat['BRICKID'][fix]))
            minicat['BRICK_OBJID'][fix] = fix_objid

        inobrickname = np.where(minicat['BRICKNAME'] == '')[0]
        if len(inobrickname) > 0:
            log.info('Inferring brickname for {:,} objects'.format(len(inobrickname)))
            minicat['BRICKNAME'][inobrickname] = brickname(minicat[racolumn][inobrickname],
                                                           minicat[deccolumn][inobrickname])
        assert(np.all(minicat['BRICKNAME'] != ''))

        # Loop over args.filenside healpixels and multiprocess over bricks.
        log.info('Gathering Tractor photometry for {:,} objects'.format(len(minicat)))

        pixels = radec2pix(args.filenside, minicat[racolumn], minicat[deccolumn])
        npix = len(set(pixels))

        missing = []
        for ipix, pixel in enumerate(set(pixels)):
            #if ipix < 42:
            #    continue
            log.info('Working on healpix {}/{} (nside={})'.format(ipix+1, npix, args.filenside))

            I = np.where(pixel == pixels)[0]
            bricknames = minicat['BRICKNAME'][I]

            log.info('Dividing the sample into {} unique bricks.'.format(len(set(bricknames))))
            mpargs = []
            for brick in set(bricknames):
                J = np.where(brick == bricknames)[0]
                mpargs.append([minicat[I[J]], racolumn, deccolumn])

            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    tractor = P.map(_tractorphot_one, mpargs)
            else:
                tractor = [_tractorphot_one(mparg) for mparg in mpargs]
            tractor = vstack(tractor)

            if len(tractor) > 0:
                # I don't understand why the Tables aren't sorted???
                srt = np.hstack([np.where(tid == tractor['TARGETID'])[0] for tid in minicat['TARGETID'][I]])
                tractor = tractor[srt]
                assert(np.all(cat['TARGETID'][I] == tractor['TARGETID']))

                # If there are any objects with missing Tractor photometry, keep track of them here.
                imiss = np.where(tractor['BRICKNAME'] == '')[0]
                if len(imiss) > 0:
                    missing.append(cat[I[imiss]])

                igood = np.where(tractor['BRICKNAME'] != '')[0]
                if len(igood) > 0:
                    if args.potential:
                        outfile = os.path.join(args.outdir, 'potential-targets', 'tractorphot', 'tractorphot-potential-nside{}-hp{:03d}-{}.fits'.format(
                            args.filenside, pixel, args.specprod))
                    else:
                        outfile = os.path.join(args.outdir, 'observed-targets', 'tractorphot', 'tractorphot-nside{}-hp{:03d}-{}.fits'.format(
                            args.filenside, pixel, args.specprod))

                    add_dependencies(tractor.meta)
                    tractor.meta['FILENSID'] = (args.filenside, 'HEALPix nside number')
                    tractor.meta['FILENEST'] = (True, 'HEALPix nested (not ring) ordering')
                    tractor.meta['FILEHPIX'] = ('{:03d}'.format(pixel), 'HEALPix number')
                    tractor.meta['EXTNAME'] = 'TRACTORPHOT'

                    log.info('Writing {:,} objects to {}'.format(len(tractor[igood]), outfile))
                    tractor[igood].write(outfile, overwrite=True)
            #del minicat

        if len(missing) > 0:
            missing = vstack(missing)

            if args.potential:
                outfile_miss = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-missing-{}.fits'.format(args.specprod))
            else:
                outfile_miss = os.path.join(args.outdir, 'ancillary', 'targetphot-missing-{}.fits'.format(args.specprod))
            add_dependencies(missing.meta)
            missing.meta['EXTNAME'] = 'TARGETPHOT'
            log.info('Writing {:,} objects to {}'.format(len(missing), outfile_miss))
            missing.write(outfile_miss, overwrite=True)

    # Validate the Tractor photometry against the original redshift catalogs.
    if args.validate_tractorphot:
        outcat = []
        outzcat = []

        missfile = os.path.join(args.outdir, 'ancillary', 'targetphot-missing-{}.fits'.format(args.specprod))
        log.info('Reading {}'.format(missfile))
        miss = Table(fitsio.read(missfile))

        zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
        #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'zpix-special-*.fits'))
        #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-sv1-other-cumulative.fits'))
        for zcatfile in zcatfiles:
            log.info('Validating {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile))
            hdr = fitsio.read_header(zcatfile, ext=1)
            zcat['SURVEY'] = hdr['SURVEY']
            zcat['PROGRAM'] = hdr['PROGRAM']

            # Toss out skies and stuck positioners.
            _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
            keep = (sky == 0) * (zcat['TARGETID'] > 0)
            zcat = zcat[keep]

            # read the appropriate Tractor catalog
            pixels = radec2pix(args.filenside, zcat['TARGET_RA'], zcat['TARGET_DEC'])
            log.info('Working on {} pixels'.format(len(set(pixels))))
            for pixel in set(pixels):
                I = np.where(pixel == pixels)[0]

                catfile = os.path.join(args.outdir, 'observed-targets', 'tractorphot', 'tractorphot-nside{}-hp{:03d}-{}.fits'.format(
                    args.filenside, pixel, args.specprod))
                if not os.path.isfile(catfile):
                    log.info('No Tractor photometry found for {} objects in catalog {}'.format(len(I), catfile))
                    # Make sure they're all in the missing catalog.
                    assert(np.sum(np.isin(miss['TARGETID'], zcat['TARGETID'][I])) == len(I))
                    continue

                log.info('Reading {} objects from {}'.format(len(I), catfile))

                _targetids = fitsio.read(catfile, columns='TARGETID')
                targetids = np.intersect1d(_targetids, zcat['TARGETID'][I])

                rows = np.where(np.isin(_targetids, targetids))[0]
                cat = Table(fitsio.read(catfile, rows=rows))
                _zcat = zcat[I][np.isin(zcat['TARGETID'][I], cat['TARGETID'])]
                J = [np.where(tid == cat['TARGETID'])[0] for tid in _zcat['TARGETID']]
                if len(J) == 0: # can happen when testing
                    continue
                J = np.hstack(J)
                cat = cat[J]
                assert(np.all(cat['TARGETID'] == _zcat['TARGETID']))

                diff = (cat['BRICKID'] != _zcat['BRICKID'])
                diff = np.logical_or(diff, cat['OBJID'] != _zcat['BRICK_OBJID'])
                #diff = np.logical_or(diff, cat['flux_g'] != _zcat['FLUX_G'])
                #diff = np.logical_or(diff, cat['flux_r'] != _zcat['FLUX_R'])
                #diff = np.logical_or(diff, cat['flux_z'] != _zcat['FLUX_Z'])
                #diff = np.logical_or(diff, cat['flux_w1'] != _zcat['FLUX_W1'])
                #diff = np.logical_or(diff, cat['flux_w2'] != _zcat['FLUX_W2'])
                diff = np.where(diff)[0]

                if np.sum(diff) > 0:
                    outcat.append(cat[diff])
                    outzcat.append(_zcat[diff])

                    #objid, brickid, release, mock, sky, gaia = decode_targetid(_zcat['TARGETID'][diff])
                    #for dd in diff:
                    #    print(_zcat['BRICKID'][dd], cat['brickid'][dd], _zcat['BRICK_OBJID'][dd], cat['objid'][dd])
                    #print(set(release))

        if len(outcat) > 0:
            _outcat = vstack(outcat)
            _outzcat = vstack(outzcat)
            # curse you, astropy!
            import numpy.ma as ma
            for col in _outzcat.colnames:
                if ma.is_masked(_outzcat[col]):
                    _outzcat[col] = ma.getdata(_outzcat[col])

            outcat = _outcat['TARGETID', 'RA', 'DEC', 'RELEASE', 'BRICKNAME', 'BRICKID', 'OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']
            outzcat = _outzcat['TARGETID', 'TARGET_RA', 'TARGET_DEC', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']

            for col in ['TARGETID', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']:
                if col == 'BRICK_OBJID':
                    outzcat.rename_column(col, 'OBJID_ZCAT')
                else:
                    outzcat.rename_column(col, '{}_ZCAT'.format(col))
            #out = hstack((outcat, outzcat))

            # interlace the columns in the output table
            tractorcols = np.array(outcat.colnames)
            zcols = np.array(outzcat.colnames)
            out = Table()
            for ii in np.arange(len(zcols)):
                out[zcols[ii]] = outzcat[zcols[ii]]
                out[tractorcols[ii]] = outcat[tractorcols[ii]]
            for col in [
                    'FA_TARGET', 'CMX_TARGET',
                    'DESI_TARGET', 'BGS_TARGET', 'MWS_TARGET',
                    'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
                    'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
                    'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
                    'SCND_TARGET',
                    'SV1_SCND_TARGET', 'SV2_SCND_TARGET', 'SV3_SCND_TARGET',
                    ]:
                out[col] = np.zeros(shape=(1,), dtype=np.int64)

            # add brick_primary, survey, program, and the targeting bits
            out['BRICK_PRIMARY'] = _outcat['BRICK_PRIMARY']
            for col in _outzcat.colnames:
                if '_TARGET' in col and col in _outzcat.colnames:
                    #print(col, _outzcat[col])
                    out[col] = _outzcat[col]

            out['SURVEY'] = _outzcat['SURVEY']
            out['PROGRAM'] = _outzcat['PROGRAM']

            outfile = os.path.join(args.outdir, 'ancillary', 'tractorphot-validate-{}.fits'.format(args.specprod))
            out.write(outfile, overwrite=True)

    # Validate the targeting photometry against the original redshift catalogs.
    if args.validate_targetphot:
        allout = []

        zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
        #zcatfiles = np.array(glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-sv1-other-cumulative.fits')))
        allsurveys = np.array([os.path.basename(zcatfile).split('-')[1] for zcatfile in zcatfiles])

        for survey in sorted(set(allsurveys)):
            log.info('Working on survey {}'.format(survey))
            # read the appropriate targetphot catalog
            catfile = os.path.join(args.outdir, 'observed-targets', 'targetphot-{}-{}.fits'.format(survey, args.specprod))
            cat = Table(fitsio.read(catfile))
            log.info('Read {:,} objects from {}'.format(len(cat), catfile))

            S = np.where(survey == allsurveys)[0]
            for zcatfile in zcatfiles[S]:
                zcat = Table(fitsio.read(zcatfile))
                log.info('Validating {:,} targets in {}'.format(len(zcat), zcatfile))

                hdr = fitsio.read_header(zcatfile, ext=1)
                zcat['SURVEY'] = hdr['SURVEY']
                zcat['PROGRAM'] = hdr['PROGRAM']

                # Toss out skies and stuck positioners.
                _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
                keep = (sky == 0) * (zcat['TARGETID'] > 0)
                zcat = zcat[keep]

                _, uindx = np.unique(zcat['TARGETID'], return_index=True)
                zcat = zcat[uindx]
                zcat = zcat[np.argsort(zcat['TARGETID'])]

                #I = np.isin(cat['TARGETID'], zcat['TARGETID'])
                I = np.hstack([np.where(tid == cat['TARGETID'])[0] for tid in zcat['TARGETID']])
                surveycat = cat[I]
                assert(np.all(surveycat['TARGETID'] == zcat['TARGETID']))

                ii = np.logical_not(np.isclose(zcat['FLUX_R'], surveycat['FLUX_R'], rtol=1e-3))
                log.info('  Found {:,} mismatches.'.format(np.sum(ii)))
                if np.sum(ii) > 0:
                    out = join(zcat['SURVEY', 'PROGRAM', 'TILEID', 'TARGETID', 'FLUX_G', 'FLUX_R', 'FLUX_Z'][ii],
                               surveycat['TARGETID', 'FLUX_G', 'FLUX_R', 'FLUX_Z'][ii], keys='TARGETID')
                    allout.append(out)

        if len(allout) > 0:
            allout = vstack(allout)
            outfile = os.path.join(args.outdir, 'ancillary', 'targetphot-validate-{}.fits'.format(args.specprod))
            allout.write(outfile, overwrite=True)

    return 0


if __name__ == '__main__':
    sys.exit(main())
