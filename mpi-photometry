#!/usr/bin/env python3

"""MPI wrapper to get a large number of image cutouts. Only run ahead of a VAC
release.

cd /global/cfs/cdirs/desi/spectro/fastspecfit/fuji/html/healpix
from glob import glob
nftot = 0
for survey in sorted(glob('*')):
    for program in glob(os.path.join(survey, '*')):
        nf = 0
        for hp100 in glob(os.path.join(program, '*')):
            for hp in glob(os.path.join(hp100, '*')):
                nf += len(glob(hp+'/tmp*.jpeg'))
        nftot += nf
        print(program, nf)
print()
print('Total: ', nftot)

"""
import pdb # for debugging

import os, sys, time
import numpy as np
import fitsio
from glob import glob
from astropy.table import Table, vstack

from desitarget import geomask
from desispec.parallel import weighted_partition
from desiutil.depend import add_dependencies

from desiutil.log import get_logger, DEBUG
log = get_logger()

ALLSURVEYS = ['sv2']
#ALLSURVEYS = ['sv1', 'sv2', 'sv3', 'cmx', 'special', 'main']

fiberassign_dir = os.path.join(os.getenv('DESI_ROOT_READONLY'), 'target', 'fiberassign', 'tiles', 'trunk')

def _read_one_zcat(args):
    """Multiprocessing wrapper."""
    return read_one_zcat(*args)

def read_one_zcat(catfile):
    """Read a single redshift catalog, e.g., ztile-sv1-dark-cumulative.fits.

    Args:
        catfile (str): full path to a given redshift catalog

    Returns an astropy.table.Table with the following columns needed to do the
    downstream matching and to enable QA: TARGETID, TILEID, TARGET_RA,
    TARGET_DEC, PETAL_LOC.

    """
    from desitarget.targets import decode_targetid

    hdr = fitsio.read_header(catfile, ext='ZCATALOG')
    survey, program = hdr['SURVEY'], hdr['PROGRAM']
    # Remove sky fibers and negative targetids (stuck fibers).
    alltargetids = fitsio.read(catfile, ext='ZCATALOG', columns='TARGETID')
    _, _, _, _, sky, _ = decode_targetid(alltargetids)
    I = np.where((sky == 0) * (alltargetids > 0))[0]
    if len(I) > 0:
        cat = Table(fitsio.read(catfile, ext='ZCATALOG', rows=I, columns=[
            'TARGETID', 'TILEID', 'TARGET_RA', 'TARGET_DEC', 'PETAL_LOC']))
        # Remove duplicate observations within a given tile; e.g., in survey=sv1,
        # program=other, tileid=80870 there are 8384 observations, but only 4192 of
        # those are unique targetids.
        indx = []
        for tileid in np.unique(cat['TILEID']):
            I = np.where(tileid == cat['TILEID'])[0]
            _, uindx = np.unique(cat['TARGETID'][I], return_index=True)
            indx.append(I[uindx])
        indx = np.hstack(indx)
        cat = cat[indx]
        cat['SURVEY'] = survey
        cat['PROGRAM'] = program
        log.info(f'Read {len(cat):,d}/{len(alltargetids):,d} objects from {catfile}.')
    return cat

def _read_one_potential_targets(args):
    """Multiprocessing wrapper."""
    return read_one_potential_targets(*args)

def read_one_potential_targets(tileid, survey, program):
    """Read the potential targets in a given fiberassign tile.

    Args:
        tileid (int): tile ID number

    Returns an astropy.table.Table with the following columns needed downstream:
    TARGETID, RA, DEC.

    """
    stileid = '{:06d}'.format(tileid)
    fiberfile = os.path.join(fiberassign_dir, stileid[:3], 'fiberassign-{}.fits.gz'.format(stileid))
    #log.info('Reading {}'.format(fiberfile))

    # old code that used POTENTIAL_ASSIGNMENTS
    #targetid = fitsio.read(fiberfile, ext='POTENTIAL_ASSIGNMENTS', columns='TARGETID')
    #targetid = np.unique(targetid)
    ## remove skies
    ##_, _, _, _, sky, _ = decode_targetid(targetid)
    #objid, brickid, release, mock, sky, gaia = decode_targetid(targetid)
    #keep = (sky == 0) * (targetid > 0)
    #out = Table()
    #if np.sum(keep) == 0:
    #    return out
    #out['TARGETID'] = targetid[keep]
    #out['TILEID'] = tileid
    #
    #targets = Table(fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC']))
    #out = join(out, targets, keys='TARGETID', join_type='left')

    out = Table(fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC']))

    # remove skies
    _, _, _, _, sky, _ = decode_targetid(out['TARGETID'])
    keep = (sky == 0) * (out['TARGETID'] > 0)
    out = out[keep]

    out['TILEID'] = tileid
    out['SURVEY'] = survey
    out['PROGRAM'] = program

    # Old code to populate the brick columns. However, this procedure is not
    # correct for secondary targets, so don't do it.
    #from desitarget.io import release_to_photsys
    #bricks = Table(fitsio.read('/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/survey-bricks.fits.gz'))
    #out['BRICKID'] = brickid[keep]
    #out['OBJID'] = objid[keep]
    #out['RELEASE'] = release[keep]
    #out['BRICKNAME'] = np.zeros(len(out), dtype=bricks['BRICKNAME'].dtype)
    #out['PHOTSYS'] = np.zeros(len(out), dtype='U1')
    #idr9 = np.where((out['RELEASE'] > 9000) * (out['RELEASE'] < 9050))[0]
    #if len(idr9) > 0:
    #    photsys = release_to_photsys(out['RELEASE'][idr9])
    #    out['PHOTSYS'][idr9] = photsys
    #    for bid in set(out['BRICKID'][idr9]):
    #        J = bid == bricks['BRICKID']
    #        I = np.where(bid == out['BRICKID'][idr9])[0]
    #        out['BRICKNAME'][idr9[I]] = bricks['BRICKNAME'][J]
    return out

def _tractorphot_one(args):
    """Multiprocessing wrapper."""
    return tractorphot_one(*args)

def tractorphot_one(cat, racolumn='TARGET_RA', deccolumn='TARGET_DEC'):
    """Simple wrapper on desispec.io.photo.gather_tractorphot."""
    from desispec.io.photo import gather_tractorphot
    tractorphot = gather_tractorphot(cat, racolumn=racolumn, deccolumn=deccolumn)
    return tractorphot

def _targetphot_one(args):
    """Multiprocessing wrapper."""
    return targetphot_one(*args)

def targetphot_one(input_cat, racolumn='TARGET_RA', deccolumn='TARGET_DEC', survey=None):
    """Simple wrapper on desispec.io.photo.gather_targetphot."""
    from desispec.io.photo import gather_targetphot

    # should be unique!
    assert(len(input_cat) == len(np.unique(input_cat['TARGETID'])))

    targetphot = gather_targetphot(input_cat, racolumn=racolumn, deccolumn=deccolumn,
                                   fiberassign_dir=fiberassign_dir)

    ## remove extraneous targeting bits -
    ## https://github.com/moustakas/desi-photometry/issues/13
    #if survey == 'main' or survey == 'special':
    #    remcols = ['CMX_TARGET',
    #               'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
    #               'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
    #               'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
    #               'SV1_SCND_TARGET', 'SV2_SCND_TARGET','SV3_SCND_TARGET']
    #    targetphot.remove_columns(remcols)

    # Can have the same targetid across different surveys and even within a
    # survey, across different tiles. So we need these columns.
    # See https://github.com/moustakas/desi-photometry/issues/3
    targetphot['SURVEY'] = input_cat['SURVEY']
    targetphot['PROGRAM'] = input_cat['PROGRAM']
    targetphot['TILEID'] = input_cat['TILEID']

    # Replace proper-motion NaNs with zeros.
    inan = np.logical_or(np.isnan(targetphot['PMRA']), np.isnan(targetphot['PMDEC']))
    if np.any(inan):
        targetphot['PMRA'][inan] = 0.0
        targetphot['PMDEC'][inan] = 0.0

    return targetphot

#def targetphot_potential(args, comm=None):
#    """Gather targeting photometry for potential targets.
#
#    """
#    if comm is None:
#        rank, size = 0, 1
#    else:
#        rank, size = comm.rank, comm.size
#
#    # rank 0 has to do a bunch of organizational work
#    t0 = time.time()
#    if rank == 0:
#        if args.potential:
#            any_outfile = glob(os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-*-{}.fits'.format(args.specprod)))
#        else:
#            any_outfile = glob(os.path.join(args.outdir, 'observed-targets', 'targetphot-*-{}.fits'.format(args.specprod)))
#
#        if len(any_outfile) > 0 and not args.overwrite:
#            log.warning('One or more targetphot output files exist; proceed with caution or use --overwrite.')
#
#        if args.potential:
#            RACOLUMN, DECCOLUMN = 'RA', 'DEC'
#            
#            # Assume that the nominal/parent zcat catalog has been previously generated.
#            parent_zcatfile = os.path.join(args.outdir, 'ancillary', f'targetphot-zcat-{args.specprod}.fits')
#            if not os.path.isfile(parent_zcatfile):
#                errmsg = f'Targeting photometric catalog {parent_zcatfile} missing.'
#                log.critical(errmsg)
#                raise IOError(errmsg)
#
#            parent_zcat = Table(fitsio.read(parent_zcatfile))
#            log.info(f'Read {len(parent_zcat):,} objects from {parent_zcatfile}.')
#
#            #tileids = np.unique(parent_zcat['TILEID'])
#            _, uindx = np.unique(parent_zcat['TILEID'], return_index=True)
#            tileids = parent_zcat['TILEID'][uindx]
#            surveys = parent_zcat['SURVEY'][uindx]
#            programs = parent_zcat['PROGRAM'][uindx]
#
#            mpargs = [[tileid, survey, program] for tileid, survey, program in zip(tileids, surveys, programs)]
#
#            if args.mp > 1:
#                with multiprocessing.Pool(args.mp) as P:
#                    zcat = P.map(_read_one_potential_targets, mpargs)
#            else:
#                zcat = [read_one_potential_targets(*mparg) for mparg in mpargs]
#            zcat = vstack(zcat)
#
#            # Do not select unique TARGETIDs because the same target can appear
#            # in different surveys; we handle uniqueness below.
#            log.info(f'Found {len(zcat):,d} objects from {len(set(zcat['TILEID'])):,d} unique tiles.')
#        else:
#            RACOLUMN, DECCOLUMN = 'TARGET_RA', 'TARGET_DEC'
#
#            # Read all the ztile catalogs in parallel.
#            print('HACK!')
#            #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
#            zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*special*-cumulative.fits'))
#            
#            mpargs = [[zcatfile] for zcatfile in zcatfiles]
#            if args.mp > 1:
#                with multiprocessing.Pool(args.mp) as P:
#                    zcat = P.map(_read_one_zcat, mpargs)
#            else:
#                zcat = [read_one_zcat(mparg[0]) for mparg in mpargs]
#
#            # Stack and then remove negative targetids and sky fibers.
#            zcat = vstack(zcat)
#            _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
#            keep = (sky == 0) * (zcat['TARGETID'] > 0)
#            if np.sum(keep) > 0:
#                log.info(f'Keeping {np.sum(keep):,d}/{len(zcat):,d} objects after removing sky targets and stuck positioners')
#                zcat = zcat[keep]
#
#            log.info(f'Read {len(zcat):,d} objects from {len(zcatfiles)} redshift catalogs.')
#
#        # Get the informational tile file for this specprod so can determine the
#        # survey.
#        infofile = os.path.join(args.reduxdir, f'tiles-{args.specprod}.csv')
#        log.info(f'Reading {infofile}')
#        if os.path.isfile(infofile):
#            tileinfo = Table.read(infofile)
#
#        # Divide the analysis by survey.
#        allout, allzcat, allminiphot = [], [], []
#        allsurveys = zcat['SURVEY'].data
#        print('HACK!!')
#        for survey in ['sv1', 'sv2', 'sv3']:
#        #for survey in sorted(set(allsurveys)):
#            log.info(f'Working on survey {survey}')
#
#            #S = np.where((survey == allsurveys) * (zcat['TILEID'] > 82400) * (zcat['TILEID'] < 82410) )[0]
#            S = np.where(survey == allsurveys)[0]
#            zcat_survey = zcat[S]
#
#            # important! sort by tile so we can stack without sorting, below
#            #zcat_survey = zcat_survey[np.argsort(zcat_survey['TILEID'])]
#
#            # Do not select unique TARGETIDs!
#            # See https://github.com/moustakas/desi-photometry/issues/3
#            #_, uindx = np.unique(zcat_survey['TARGETID'], return_index=True)
#            #zcat_survey = zcat_survey[uindx]
#            #zcat_survey = zcat_survey[np.argsort(zcat_survey['TARGETID'])]
#
#            utiles = np.unique(zcat_survey['TILEID'])
#            log.info(f'Found {len(zcat_survey):,d} TARGETIDs and {len(utiles):,d} unique tiles from survey: {survey}')
#
#            # Split potential MAIN targets into nside=2 healpixels because there
#            # are too many of them.
#            if args.potential and survey == 'main':
#                miniphot = []
#                
#                T = tileinfo[np.hstack([np.where(_tileid == tileinfo['TILEID'])[0] for _tileid in utiles])]
#                assert(np.all(T['TILEID'] == utiles))
#    
#                pixels = radec2pix(args.nside_targetphot, T['TILERA'], T['TILEDEC'])
#                npix = len(set(pixels))
#
#                for ipix, pixel in enumerate(sorted(set(pixels))):
#                    outfile = os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-nside{}-hp{:02d}-{}-{}.fits'.format(
#                        args.nside_targetphot, pixel, survey, args.specprod))
#
#                    if False:
#                        print('Temporarily reading {}'.format(outfile))
#                        miniphot.append(Table(fitsio.read(outfile, columns=['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID'])))
#                    else:
#                        Pix = np.where(pixel == pixels)[0]
#        
#                        mpargs = []
#                        zcat_pixel = []
#                        for tileid in T['TILEID'][Pix]:
#                            I = zcat_survey['TILEID'] == tileid
#                            zcat_pixel.append(zcat_survey[I])
#                            mpargs.append([zcat_survey[I], RACOLUMN, DECCOLUMN, survey])
#                        zcat_pixel = vstack(zcat_pixel)
#            
#                        t0 = time.time()
#                        if args.mp > 1:
#                            with multiprocessing.Pool(args.mp) as P:
#                                out = P.map(_targetphot_one, mpargs)
#                        else:
#                            out = [targetphot_one(*mparg) for mparg in mpargs]
#                        log.info('Total time for gather_targetphot = {:.3f} min'.format((time.time()-t0)/60))
#    
#                        # stack, sort, and write out
#                        try:
#                            out = vstack(out)
#                            assert(np.all(out['TARGETID'] == zcat_pixel['TARGETID']))
#                        except:
#                            errmsg = 'TARGETID mismatch!'
#                            log.critical(errmsg)
#                            raise IOError(errmsg)
#
#                        add_dependencies(out.meta, envvar_names=['DESI_ROOT'])
#                        out.meta['SURVEY'] = (survey, 'survey name')
#                        out.meta['FILENSID'] = (args.nside_targetphot, 'HEALPix nside number')
#                        out.meta['FILENEST'] = (True, 'HEALPix nested (not ring) ordering')
#                        out.meta['FILEHPIX'] = ('{:02d}'.format(pixel), 'HEALPix number')
#                        out.meta['EXTNAME'] = 'TARGETPHOT'
#                        log.info('Writing {:,} objects to {}'.format(len(out), outfile))
#                        out.write(outfile, overwrite=True)
#    
#                        miniphot.append(out['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID'])#, 'RA', 'DEC']
#
#                print('Stacking miniphot...')
#                miniphot = vstack(miniphot)
#
#                # we somehow get out of order...
#                # https://stackoverflow.com/questions/68085572/sort-numpy-array-by-another-array
#                idx1 = np.argsort(zcat_survey['TARGETID'])
#                idx2 = np.argsort(miniphot['TARGETID'])
#                idx1_inv = np.argsort(idx1)
#                miniphot = miniphot[idx2][idx1_inv]
#                
#                # so slow!!
#                #srt = np.hstack([np.where(tid == miniphot['TARGETID'])[0] for tid in zcat_survey['TARGETID']])
#                #miniphot = miniphot[srt]
#            else:
#                if args.potential:
#                    outfile = os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-{}-{}.fits'.format(survey, args.specprod))
#                else:
#                    outfile = os.path.join(args.outdir, 'observed-targets', 'targetphot-{}-{}.fits'.format(survey, args.specprod))
#
#                npertile = [np.sum(zcat_survey['TILEID'] == tileid) for tileid in utiles]
#                groups = weighted_partition(npertile, size)
#
#        if comm:
#            groups = comm.bcast(groups, root=0)
#
#        for ii in groups[rank]:
#            log.debug(f'Rank {rank} started at {time.asctime()}')
#            sys.stdout.flush()
#
#
#            
#
#                mpargs = []
#                for tileid in utiles:
#                    I = zcat_survey['TILEID'] == tileid
#                    mpargs.append([zcat_survey[I], RACOLUMN, DECCOLUMN, survey])
#    
#                t0 = time.time()
#                if args.mp > 1:
#                    with multiprocessing.Pool(args.mp) as P:
#                        out = P.map(_targetphot_one, mpargs)
#                else:
#                    out = [targetphot_one(*mparg) for mparg in mpargs]
#                log.info('Total time for gather_targetphot = {:.3f} min'.format((time.time()-t0)/60))
#    
#                # stack, sort, and write out
#                out = vstack(out)
#                assert(np.all(out['TARGETID'] == zcat_survey['TARGETID']))
#
#                miniphot = out['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID']#, 'RA', 'DEC']
#
#                # remove extraneous targeting bits -
#                # https://github.com/moustakas/desi-photometry/issues/13
#                if survey == 'main' or survey == 'special':                
#                    remcols = ['CMX_TARGET',
#                               'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
#                               'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
#                               'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
#                               'SV1_SCND_TARGET', 'SV2_SCND_TARGET','SV3_SCND_TARGET']
#                    out.remove_columns(remcols)
#            
#                add_dependencies(out.meta, envvar_names=['DESI_ROOT'])
#                out.meta['SURVEY'] = (survey, 'survey name')
#                out.meta['EXTNAME'] = 'TARGETPHOT'
#                log.info('Writing {:,} objects to {}'.format(len(out), outfile))
#                out.write(outfile, overwrite=True)
#
#                out['SURVEY'] = survey
#                #if survey == 'main' or survey == 'special':
#                #    allout_mainspecial.append(out)
#                #else:
#                #    allout_cmxsv.append(out)
#                allout.append(out)
#                del out
#    
#            # Also write out a "mini" targetphot catalog and the redshift
#            # catalog (with a minimal number of columns) used to generate the
#            # targetphot file. Note that this intermediate catalog is needed /
#            # used to do positional Tractor photometry gathering, below.
#            if args.potential:
#                outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-zcat-{}-{}.fits'.format(survey, args.specprod))
#                outfile_miniphot = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-miniphot-{}-{}.fits'.format(survey, args.specprod))
#
#                try:
#                    assert(np.all(miniphot['TARGETID'] == zcat_survey['TARGETID']))
#                except:
#                    errmsg = 'TARGETID mismatch!'
#                    log.critical(errmsg)
#                    raise IOError(errmsg)
#                    
#                add_dependencies(miniphot.meta, envvar_names=['DESI_ROOT'])
#                miniphot.meta['SURVEY'] = (survey, 'survey name')
#                miniphot.meta['EXTNAME'] = 'MINIPHOT'
#                log.info('Writing {:,} objects to {}'.format(len(miniphot), outfile_miniphot))
#                miniphot.write(outfile_miniphot, overwrite=True)
#                
#            else:
#                outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-zcat-{}-{}.fits'.format(survey, args.specprod))
#    
#            add_dependencies(zcat_survey.meta, envvar_names=['DESI_ROOT'])
#            zcat_survey.meta['SURVEY'] = (survey, 'survey name')
#            zcat_survey.meta['EXTNAME'] = 'ZCATALOG'
#            log.info('Writing {:,} objects to {}'.format(len(zcat_survey), outfile_zcat))
#            zcat_survey.write(outfile_zcat, overwrite=True)
#
#            #if survey == 'main' or survey == 'special':
#            #    allzcat_mainspecial.append(zcat_survey)
#            #else:
#            #    allzcat_cmxsv.append(zcat_survey)
#            allzcat.append(zcat_survey)
#            allminiphot.append(miniphot)
#
#        #if len(allzcat_cmxsv) > 0 or len(allzcat_mainspecial) > 0:# and not args.potential:
#        if len(allzcat) > 0:
#            if args.potential:
#                outfile = os.path.join(args.outdir, 'potential-targets', 'targetphot-potential-{}.fits'.format(args.specprod))
#                outfile_mini = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-miniphot-{}.fits'.format(args.specprod))
#                outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-potential-zcat-{}.fits'.format(args.specprod))
#            else:
#                outfile = os.path.join(args.outdir, 'observed-targets', 'targetphot-{}.fits'.format(args.specprod))
#                outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-zcat-{}.fits'.format(args.specprod))
#
#            allzcat = vstack(allzcat, metadata_conflicts='silent')
#            allzcat.meta.pop('SURVEY')
#
#            log.info('Writing {:,} objects to {}'.format(len(allzcat), outfile_zcat))
#            allzcat.write(outfile_zcat, overwrite=True)
#
#            allout = vstack(allout, metadata_conflicts='silent')
#            try:
#                assert(np.all(allout['TARGETID'] == allzcat['TARGETID']))
#            except:
#                errmsg = 'TARGETID mismatch!'
#                log.critical(errmsg)
#                raise IOError(errmsg)
#            
#            allout.meta.pop('SURVEY')
#            #allout.meta['EXTNAME'] = 'TARGETPHOT'
#            log.info('Writing {:,} objects to {}'.format(len(allout), outfile))
#            allout.write(outfile, overwrite=True)
#
#            if args.potential:
#                allminiphot = vstack(allminiphot, metadata_conflicts='silent')
#                try:
#                    assert(np.all(allminiphot['TARGETID'] == allzcat['TARGETID']))
#                except:
#                    errmsg = 'TARGETID mismatch!'
#                    log.critical(errmsg)
#                    raise IOError(errmsg)
#                    
#                add_dependencies(allminiphot.meta, envvar_names=['DESI_ROOT'])
#                allminiphot.meta['EXTNAME'] = 'MINIPHOT'
#                log.info('Writing {:,} objects to {}'.format(len(allminiphot), outfile_mini))
#                allminiphot.write(outfile_mini, overwrite=True)

def main():
    """Main wrapper.

    """
    import argparse
    from desispec.io import specprod_root
    
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--specprod', type=str, required=True, help='Spectroscopic production; output file prefix.')
    parser.add_argument('--reduxdir', type=str, help='spectro redux base dir overrides $DESI_SPECTRO_REDUX/$SPECPROD')
    parser.add_argument('--outdir', default='$PSCRATCH/lsdr9', type=str, help='Base output data directory.')

    parser.add_argument('--potential', action='store_true', help='Gather photometry for the potential targets.')
    parser.add_argument('--targetphot', action='store_true', help='Build the photo-targets catalogs.')
    parser.add_argument('--tractorphot', action='store_true', help='Build the photo-tractor catalogs.')

    parser.add_argument('--validate-targetphot', action='store_true', help='Validate the targetphot catalogs.')
    parser.add_argument('--validate-tractorphot', action='store_true', help='Validate the photo-tractor catalogs.')
    
    parser.add_argument('--mp', type=int, default=1, help='Number of multiprocessing processes per MPI rank or node.')
    parser.add_argument('--nside-tractorphot', type=int, default=4, help='healpix nside for tractorphot catalogs.')
    parser.add_argument('--nside-targetphot', type=int, default=2, help='healpix nside for targetphot-main catalogs.')

    parser.add_argument('--plan', action='store_true', help='Plan how many nodes to use and how to distribute the targets.')
    parser.add_argument('--nompi', action='store_true', help='Do not use MPI parallelism.')
    parser.add_argument('--dry-run', action='store_true', help='Generate but do not run commands.')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite any existing output files.')
    
    args = parser.parse_args()
    log = get_logger()    

    args.outdir = os.path.expandvars(args.outdir)

    if args.reduxdir is None:
        args.reduxdir = specprod_root()
        
    for subdir in ['observed-targets', 'potential-targets']:
        if not os.path.isdir(os.path.join(args.outdir, subdir)):
            os.makedirs(os.path.join(args.outdir, subdir), exist_ok=True)
        if not os.path.isdir(os.path.join(args.outdir, subdir, 'tractorphot')):
            os.makedirs(os.path.join(args.outdir, subdir, 'tractorphot'), exist_ok=True)
    if not os.path.isdir(os.path.join(args.outdir, 'ancillary')):
        os.makedirs(os.path.join(args.outdir, 'ancillary'), exist_ok=True)

    if args.nompi:
        comm = None
    else:
        try:
            from mpi4py import MPI
            comm = MPI.COMM_WORLD
        except ImportError:
            comm = None

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    # https://docs.nersc.gov/development/languages/python/parallel-python/#use-the-spawn-start-method
    if args.mp > 1 and 'NERSC_HOST' in os.environ:
        import multiprocessing
        multiprocessing.set_start_method('spawn')

    if args.targetphot:
        if args.potential:
            #targetphot_potential(args, comm=comm)
            pass
        else:
            if rank == 0:
                any_outfile = glob(os.path.join(args.outdir, 'observed-targets', f'targetphot-*-{args.specprod}.fits'))
                if len(any_outfile) > 0 and not args.overwrite:
                    log.warning('One or more targetphot output files exist; proceed with caution or use --overwrite.')

                allzcat = []
                allminiphot = []
                alltargetphot = []

                t0 = time.time()

            # Divide by survey.
            for survey in ALLSURVEYS:
                # Read all the ztile programs/catalogs in parallel on rank 0.
                if rank == 0:
                    log.info(f'Working on survey {survey}')
                    targetphot_outfile = os.path.join(args.outdir, 'observed-targets', f'targetphot-{survey}-{args.specprod}.fits')
                    zcat_outfile = os.path.join(args.outdir, 'ancillary', f'targetphot-zcat-{survey}-{args.specprod}.fits')
                
                    zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', f'ztile-{survey}-*-cumulative.fits'))
                    
                    mpargs = [[zcatfile] for zcatfile in zcatfiles]
                    if args.mp > 1:
                        with multiprocessing.Pool(args.mp) as P:
                            zcat_survey = P.map(_read_one_zcat, mpargs)
                    else:
                        zcat_survey = [read_one_zcat(mparg[0]) for mparg in mpargs]
                    zcat_survey = vstack(zcat_survey)

                    utiles = np.unique(zcat_survey['TILEID'])
                    log.info(f'Found {len(zcat_survey):,d} TARGETIDs and {len(utiles):,d} unique tiles from survey: {survey}')
    
                    npertile = [np.sum(zcat_survey['TILEID'] == tileid) for tileid in utiles]
                    groups = weighted_partition(npertile, size)

                    # broadcast the work to the other ranks
                    if comm:
                        zcat_survey = comm.bcast(zcat_survey, root=0)
                        utiles = comm.bcast(utiles, root=0)
                        groups = comm.bcast(groups, root=0)
                    
                log.debug(f'Rank {rank} started at {time.asctime()} with {len(groups[rank])} tiles.')

                targetphot_survey = []
                for tileid in utiles[groups[rank]]:
                    log.info(f'Rank {rank} is working on tile {tileid}.')
                    I = zcat_survey['TILEID'] == tileid
                    targetphot_onetile = targetphot_one(zcat_survey[I], 'TARGET_RA', 'TARGET_DEC', survey)
                    #print(len(zcat_survey[I]), len(targetphot_onetile))
                    try:
                        assert(np.all(targetphot_onetile['TARGETID'] == zcat_survey[I]['TARGETID']))
                    except:
                        pdb.set_trace()
                    targetphot_survey.append(targetphot_onetile)
                targetphot_survey = vstack(targetphot_survey)

                if comm is not None:
                    comm.barrier() # make sure all ranks are done

                # stack, sort, and write out on rank 0
                if rank == 0:
                    if comm is not None:
                        targetphot_survey = comm.gather(targetphot_survey, root=0)
                        targetphot_survey = vstack(targetphot_survey)

                    # Different ranks can finish at different times and
                    # targetids can repeat across tiles, so we need to loop to
                    # make sure targetphot_survey is sorted relative to
                    # zcat_survey.
                    S = []
                    for tileid in utiles:
                        I = zcat_survey['TILEID'] == tileid
                        J = targetphot_survey['TILEID'] == tileid
                        try:
                            S.append(geomask.match_to(zcat_survey[I]['TARGETID'].data, targetphot_survey['TARGETID'].data))
                        except:
                            pdb.set_trace()
                    S = np.hstack(S)
                    targetphot_survey = targetphot_survey[S]

                    try:
                        assert(np.all(targetphot_survey['TARGETID'] == zcat_survey['TARGETID']))
                    except:
                        pdb.set_trace()

                    pdb.set_trace()                        
                    
                    # build a super-stack of catalogs
                    allminiphot.append(targetphot_survey['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID'])

                    # remove extraneous targeting bits -
                    # https://github.com/moustakas/desi-photometry/issues/13
                    if survey == 'main' or survey == 'special':                
                        remcols = ['CMX_TARGET',
                                   'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
                                   'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
                                   'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
                                   'SV1_SCND_TARGET', 'SV2_SCND_TARGET','SV3_SCND_TARGET']
                        targetphot_survey.remove_columns(remcols)
                
                    add_dependencies(targetphot_survey.meta, envvar_names=['DESI_ROOT', 'DESI_ROOT_READONLY'])
                    targetphot_survey.meta['SURVEY'] = (survey, 'survey name')
                    targetphot_survey.meta['EXTNAME'] = 'TARGETPHOT'
                    log.info(f'Writing {len(targetphot_survey):,d} objects to {targetphot_outfile}')
                    targetphot_survey.write(targetphot_outfile, overwrite=True)

                    targetphot_survey['SURVEY'] = survey                    
                    alltargetphot.append(targetphot_survey)
                    del targetphot_survey

                    add_dependencies(zcat_survey.meta, envvar_names=['DESI_ROOT', 'DESI_ROOT_READONLY'])
                    zcat_survey.meta['SURVEY'] = (survey, 'survey name')
                    zcat_survey.meta['EXTNAME'] = 'ZCATALOG'
                    log.info(f'Writing {len(zcat_survey):,d} objects to {zcat_outfile}')
                    zcat_survey.write(zcat_outfile, overwrite=True)

                    allzcat.append(zcat_survey)
                    del zcat_survey

            log.info(f'Total time for all surveys = {(time.time()-t0)/60.:.3f} min')
            pdb.set_trace()
    
            if len(allzcat) > 0:
                outfile = os.path.join(args.outdir, 'observed-targets', 'targetphot-{}.fits'.format(args.specprod))
                outfile_zcat = os.path.join(args.outdir, 'ancillary', 'targetphot-zcat-{}.fits'.format(args.specprod))
    
                allzcat = vstack(allzcat, metadata_conflicts='silent')
                allzcat.meta.pop('SURVEY')
    
                log.info('Writing {:,} objects to {}'.format(len(allzcat), outfile_zcat))
                allzcat.write(outfile_zcat, overwrite=True)
    
                allout = vstack(allout, metadata_conflicts='silent')
                try:
                    assert(np.all(allout['TARGETID'] == allzcat['TARGETID']))
                except:
                    errmsg = 'TARGETID mismatch!'
                    log.critical(errmsg)
                    raise IOError(errmsg)
                
                allout.meta.pop('SURVEY')
                #allout.meta['EXTNAME'] = 'TARGETPHOT'
                log.info('Writing {:,} objects to {}'.format(len(allout), outfile))
                allout.write(outfile, overwrite=True)

                    
    #if args.tractorphot:
    #    if args.potential:
    #        tractorphot_potential(args, comm=comm)
    #    else:
    #        tractorphot(args, comm=comm)

if __name__ == '__main__':
    main()
