#!/usr/bin/env python3

"""Match data release redshift catalogs against the original photometric
(target) catalogs.

Set up the software dependencies:
  salloc -N 1 -C cpu -A desi -t 01:00:00 --qos interactive
  source /dvs_ro/common/software/desi/desi_environment.sh 23.6
  for package in desispec; do
    echo Loading local check-out of $package
    module unload $package
    export PATH=$HOME/code/desihub/$package/bin:${PATH}
    export PYTHONPATH=$HOME/code/desihub/$package/py:${PYTHONPATH}
  done



srun --nodes 1 --ntasks 16 --cpus-per-task 8 --cpu-bind=none $HOME/code/desihub/desi-photometry/mpi-photometry --outdir $PSCRATCH/lsdr9/fuji --reduxdir $DESI_ROOT_READONLY/spectro/redux/fuji --specprod fuji --mp 4 --targetphot

srun --nodes 1 --ntasks 16 --cpus-per-task 8 --cpu-bind=none ./desi-photometry/mpi-photometry --outdir $PSCRATCH/lsdr9/fuji --reduxdir $DESI_ROOT_READONLY/spectro/redux/fuji --specprod fuji --mp 4 --targetphot



Sequence of commands for a given release (here, iron)
1. Gather targeting photometry from the individual redshift catalogs.

   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir ${DESI_ROOT}/spectro/redux/iron -o ${SCRATCH}/lsdr9/iron --specprod iron --mp 128 --targetphot

   Output files:
    targetphot-iron.fits
    targetphot-zcat-iron.fits
    targetphot-missing-iron.fits

2. Gather Tractor photometry.

   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir ${DESI_ROOT}/spectro/redux/iron -o ${SCRATCH}/lsdr9/iron --specprod iron --mp 128 --tractorphot

   Output files:
     tractorphot-nside4-hp???-iron.fits

3. Gather targeting and Tractor photometry for *potential* targets (going back to the original fiberassign files).

   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir ${DESI_ROOT}/spectro/redux/iron -o ${SCRATCH}/lsdr9/iron --specprod iron --mp 128 --targetphot --potential
   time /global/homes/i/ioannis/code/desihub/desi-photometry/lsdr9-photometry --reduxdir ${DESI_ROOT}/spectro/redux/iron -o ${SCRATCH}/lsdr9/iron --specprod iron --mp 128 --tractorphot --potential

   Output files:
     targetphot-potential-iron.fits
     targetphot-potential-zcat-iron.fits
     targetphot-potential-missing-iron.fits
     tractorphot-potential-nside4-hp???-iron.fits

"""
import pdb # for debugging

import os, sys, time
import numpy as np
import fitsio
from glob import glob
from astropy.table import Table, vstack

from desitarget import geomask
from desispec.parallel import weighted_partition
from desiutil.depend import add_dependencies

from desiutil.log import get_logger, DEBUG
log = get_logger()

ALLSURVEYS = ['sv1', 'sv2', 'sv3', 'cmx', 'special', 'main']
#ALLSURVEYS = ['main']

def _read_one_zcat(args):
    """Multiprocessing wrapper."""
    return read_one_zcat(*args)

def read_one_zcat(catfile):
    """Read a single redshift catalog, e.g., ztile-sv1-dark-cumulative.fits.

    Args:
        catfile (str): full path to a given redshift catalog

    Returns an astropy.table.Table with the following columns needed to do the
    downstream matching and to enable QA: TARGETID, TILEID, TARGET_RA,
    TARGET_DEC, PETAL_LOC.

    """
    from desitarget.targets import decode_targetid

    hdr = fitsio.read_header(catfile, ext='ZCATALOG')
    survey, program = hdr['SURVEY'], hdr['PROGRAM']
    # Remove sky fibers and negative targetids (stuck fibers).
    alltargetids = fitsio.read(catfile, ext='ZCATALOG', columns='TARGETID')
    _, _, _, _, sky, _ = decode_targetid(alltargetids)
    I = np.where((sky == 0) * (alltargetids > 0))[0]
    if len(I) > 0:
        cat = Table(fitsio.read(catfile, ext='ZCATALOG', rows=I, columns=[
            'TARGETID', 'TILEID', 'TARGET_RA', 'TARGET_DEC', 'PETAL_LOC']))
        # Remove duplicate observations within a given tile; e.g., in survey=sv1,
        # program=other, tileid=80870 there are 8384 observations, but only 4192 of
        # those are unique targetids.
        indx = []
        for tileid in np.unique(cat['TILEID']):
            I = np.where(tileid == cat['TILEID'])[0]
            _, uindx = np.unique(cat['TARGETID'][I], return_index=True)
            indx.append(I[uindx])
        indx = np.hstack(indx)
        cat = cat[indx]
        cat['SURVEY'] = survey
        cat['PROGRAM'] = program
        log.info(f'Read {len(cat):,d}/{len(alltargetids):,d} objects from {catfile}.')
    return cat

def _read_one_potential_targets(args):
    """Multiprocessing wrapper."""
    return read_one_potential_targets(*args)

def read_one_potential_targets(tileid, survey, program):
    """Read the potential targets in a given fiberassign tile.

    Args:
        tileid (int): tile ID number

    Returns an astropy.table.Table with the following columns needed downstream:
    TARGETID, RA, DEC.

    """
    from desitarget.targets import decode_targetid

    fiberassign_dir = os.path.join(os.getenv('DESI_ROOT_READONLY'), 'target', 'fiberassign', 'tiles', 'trunk')
    
    stileid = '{:06d}'.format(tileid)
    fiberfile = os.path.join(fiberassign_dir, stileid[:3], 'fiberassign-{}.fits.gz'.format(stileid))
    #log.info('Reading {}'.format(fiberfile))

    # old code that used POTENTIAL_ASSIGNMENTS
    #targetid = fitsio.read(fiberfile, ext='POTENTIAL_ASSIGNMENTS', columns='TARGETID')
    #targetid = np.unique(targetid)
    ## remove skies
    ##_, _, _, _, sky, _ = decode_targetid(targetid)
    #objid, brickid, release, mock, sky, gaia = decode_targetid(targetid)
    #keep = (sky == 0) * (targetid > 0)
    #out = Table()
    #if np.sum(keep) == 0:
    #    return out
    #out['TARGETID'] = targetid[keep]
    #out['TILEID'] = tileid
    #
    #targets = Table(fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC']))
    #out = join(out, targets, keys='TARGETID', join_type='left')

    out = Table(fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC']))

    # remove skies
    _, _, _, _, sky, _ = decode_targetid(out['TARGETID'])
    keep = (sky == 0) * (out['TARGETID'] > 0)
    out = out[keep]

    out['TILEID'] = tileid
    out['SURVEY'] = survey
    out['PROGRAM'] = program

    # Old code to populate the brick columns. However, this procedure is not
    # correct for secondary targets, so don't do it.
    #from desitarget.io import release_to_photsys
    #bricks = Table(fitsio.read('/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/survey-bricks.fits.gz'))
    #out['BRICKID'] = brickid[keep]
    #out['OBJID'] = objid[keep]
    #out['RELEASE'] = release[keep]
    #out['BRICKNAME'] = np.zeros(len(out), dtype=bricks['BRICKNAME'].dtype)
    #out['PHOTSYS'] = np.zeros(len(out), dtype='U1')
    #idr9 = np.where((out['RELEASE'] > 9000) * (out['RELEASE'] < 9050))[0]
    #if len(idr9) > 0:
    #    photsys = release_to_photsys(out['RELEASE'][idr9])
    #    out['PHOTSYS'][idr9] = photsys
    #    for bid in set(out['BRICKID'][idr9]):
    #        J = bid == bricks['BRICKID']
    #        I = np.where(bid == out['BRICKID'][idr9])[0]
    #        out['BRICKNAME'][idr9[I]] = bricks['BRICKNAME'][J]
    return out

def _tractorphot_one(args):
    """Multiprocessing wrapper."""
    return tractorphot_one(*args)

def tractorphot_one(cat, racolumn='TARGET_RA', deccolumn='TARGET_DEC'):
    """Simple wrapper on desispec.io.photo.gather_tractorphot."""
    from desispec.io.photo import gather_tractorphot
    tractorphot = gather_tractorphot(cat, racolumn=racolumn, deccolumn=deccolumn)
    return tractorphot

def _targetphot_onetile(args):
    """Multiprocessing wrapper."""
    return targetphot_onetile(*args)

def targetphot_onetile(input_cat, racolumn='TARGET_RA', deccolumn='TARGET_DEC', survey=None):
    """Simple wrapper on desispec.io.photo.gather_targetphot."""
    from desispec.io.photo import gather_targetphot

    # should be unique!
    assert(len(input_cat) == len(np.unique(input_cat['TARGETID'])))

    targetphot = gather_targetphot(input_cat, racolumn=racolumn, deccolumn=deccolumn, verbose=False)

    ## remove extraneous targeting bits -
    ## https://github.com/moustakas/desi-photometry/issues/13
    #if survey == 'main' or survey == 'special':
    #    remcols = ['CMX_TARGET',
    #               'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
    #               'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
    #               'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
    #               'SV1_SCND_TARGET', 'SV2_SCND_TARGET','SV3_SCND_TARGET']
    #    targetphot.remove_columns(remcols)

    # Can have the same targetid across different surveys and even within a
    # survey, across different tiles. So we need these columns.
    # See https://github.com/moustakas/desi-photometry/issues/3
    targetphot['SURVEY'] = input_cat['SURVEY']
    targetphot['PROGRAM'] = input_cat['PROGRAM']
    targetphot['TILEID'] = input_cat['TILEID']

    # Replace proper-motion NaNs with zeros.
    inan = np.logical_or(np.isnan(targetphot['PMRA']), np.isnan(targetphot['PMDEC']))
    if np.any(inan):
        targetphot['PMRA'][inan] = 0.0
        targetphot['PMDEC'][inan] = 0.0

    return targetphot

def write_targetphot(targetphot, zcat, survey=None, targetphot_outfile=None,
                     zcat_outfile=None, miniphot_outfile=None, sort=True):
    """Simple wrapper to write out a targetphot catalog and its ancillary catalogs.

    """
    # Optionally row-match targetphot_survey to zcat_survey, taking into account
    # that, e.g., since different ranks can finish at different times and
    # targetids can repeat across tiles, we need to sort on a tileid-targetid
    # key.
    if sort:
        zkey = [f'{tile}-{tid}' for tile, tid in zip(zcat['TILEID'], zcat['TARGETID'])]
        tkey = [f'{tile}-{tid}' for tile, tid in zip(targetphot['TILEID'], targetphot['TARGETID'])]
        zsrt = geomask.match_to(tkey, zkey)
        targetphot = targetphot[zsrt]
        assert(np.all(targetphot['TARGETID'] == zcat['TARGETID']))
    
    # remove extraneous targeting bits -
    # https://github.com/moustakas/desi-photometry/issues/13
    if survey == 'main' or survey == 'special':                
        remcols = ['CMX_TARGET',
                   'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
                   'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
                   'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
                   'SV1_SCND_TARGET', 'SV2_SCND_TARGET','SV3_SCND_TARGET']
        for remcol in remcols:
            if remcol in targetphot.colnames:
                targetphot.remove_column(remcol)

    if targetphot_outfile is not None:                
        add_dependencies(targetphot.meta, envvar_names=['DESI_ROOT', 'DESI_ROOT_READONLY'])
        targetphot.meta['SURVEY'] = (survey, 'survey name')
        targetphot.meta['EXTNAME'] = 'TARGETPHOT'
        log.info(f'Writing {len(targetphot):,d} objects to {targetphot_outfile}')
        targetphot.write(targetphot_outfile, overwrite=True)

    targetphot['SURVEY'] = survey                    

    if zcat_outfile is not None:
        add_dependencies(zcat.meta, envvar_names=['DESI_ROOT', 'DESI_ROOT_READONLY'])
        zcat.meta['SURVEY'] = (survey, 'survey name')
        zcat.meta['EXTNAME'] = 'ZCATALOG'
        log.info(f'Writing {len(zcat):,d} objects to {zcat_outfile}')
        zcat.write(zcat_outfile, overwrite=True)

    # build a "mini" ancillary catalog across all surveys
    if miniphot_outfile is not None:
        miniphot = targetphot['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID']

        add_dependencies(miniphot.meta, envvar_names=['DESI_ROOT', 'DESI_ROOT_READONLY'])
        miniphot.meta['SURVEY'] = (survey, 'survey name')
        miniphot.meta['EXTNAME'] = 'MINIPHOT'
        log.info(f'Writing {len(miniphot):,d} objects to {miniphot_outfile}')
        miniphot.write(miniphot_outfile, overwrite=True)
    else:
        miniphot = None
        
    return targetphot, zcat, miniphot

def targetphot_observed(specprod, reduxdir, outdir, comm=None, mp=1, overwrite=False):
    """Gather targeting photometry for observed targets.

    """
    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    RACOLUMN, DECCOLUMN = 'TARGET_RA', 'TARGET_DEC'

    if rank == 0:
        any_outfile = glob(os.path.join(outdir, 'observed-targets', f'targetphot-*-{specprod}.fits'))
        if len(any_outfile) > 0 and not overwrite:
            log.warning('One or more targetphot output files exist; proceed with caution or use --overwrite.')

        allzcat = []
        alltargetphot = []
        #allminiphot = []

        tall = time.time()

    # Divide by survey.
    for survey in ALLSURVEYS:
        tone = time.time()
        
        zcatfiles = glob(os.path.join(reduxdir, 'zcatalog', f'ztile-{survey}-*-cumulative.fits'))
        if len(zcatfiles) == 0:
            log.info(f'No redshift catalogs for survey:specprod {survey}:{specprod}')
            continue

        # initialize
        zcat_survey = Table()
        utiles = np.array([])
        groups = [np.array([])]
        
        # Read all the ztile programs/catalogs in parallel on rank 0.
        if rank == 0:
            log.info(f'Working on survey {survey}')
            
            targetphot_outfile = os.path.join(outdir, 'observed-targets', f'targetphot-{survey}-{specprod}.fits')
            zcat_outfile = os.path.join(outdir, 'ancillary', f'targetphot-zcat-{survey}-{specprod}.fits')
        
            mpargs = [[zcatfile] for zcatfile in zcatfiles]
            if mp > 1:
                import multiprocessing
                with multiprocessing.Pool(mp) as P:
                    zcat_survey = P.map(_read_one_zcat, mpargs)
            else:
                zcat_survey = [read_one_zcat(mparg[0]) for mparg in mpargs]
            zcat_survey = vstack(zcat_survey)

            print('HACK!')
            utiles = np.unique(zcat_survey['TILEID'])
            if len(utiles) > 1:
                I = np.random.choice(len(utiles), 2, replace=False)
                zcat_survey = zcat_survey[np.isin(zcat_survey['TILEID'], utiles[I])]
            #I = np.random.choice(len(zcat_survey), 20, replace=False)
            #zcat_survey = zcat_survey[I]

            utiles, npertile = np.unique(zcat_survey['TILEID'], return_counts=True)
            log.info(f'Found {len(zcat_survey):,d} TARGETIDs and {len(utiles):,d} unique tiles from survey: {survey}')

            groups = weighted_partition(npertile, size)

        # broadcast the work to the other ranks
        if comm:
            zcat_survey = comm.bcast(zcat_survey, root=0)
            utiles = comm.bcast(utiles, root=0)
            groups = comm.bcast(groups, root=0)

        log.info(f'Rank {rank} started at {time.asctime()} with {len(groups[rank])} tiles.')

        targetphot_survey = []
        for tileid in utiles[groups[rank]]:
            log.info(f'Rank {rank} is working on tile {tileid}')
            I = zcat_survey['TILEID'] == tileid
            _targetphot_onetile = targetphot_onetile(zcat_survey[I], RACOLUMN, DECCOLUMN, survey)
            #print(len(zcat_survey[I]), len(_targetphot_onetile))
            assert(np.all(_targetphot_onetile['TARGETID'] == zcat_survey[I]['TARGETID']))
            targetphot_survey.append(_targetphot_onetile)
        if len(targetphot_survey) > 0:
            targetphot_survey = vstack(targetphot_survey)

        if comm:
            targetphot_survey = comm.gather(targetphot_survey, root=0)

        # sort and write out on rank 0
        if rank == 0:
            if len(targetphot_survey) == 0:
                errmsg = 'No targeting catalogs were generated!'
                log.critical(errmsg)
                raise ValueError(errmsg)
            targetphot_survey = vstack(targetphot_survey)

            targetphot_survey, zcat_survey, _ = write_targetphot(targetphot_survey, zcat_survey,
                                                                 targetphot_outfile=targetphot_outfile,
                                                                 zcat_outfile=zcat_outfile,
                                                                 survey=survey, sort=True)
            assert(np.all(targetphot_survey['TARGETID'] == zcat_survey['TARGETID']))            

            alltargetphot.append(targetphot_survey)
            allzcat.append(zcat_survey)
            del zcat_survey, targetphot_survey

            log.info(f'Total time for survey {survey}: {(time.time()-tone)/60.:.3f} min')

        log.info(f'Rank {rank} is all done!')

    # write out the final super-catalogs
    if rank == 0:
        targetphot_outfile = os.path.join(outdir, 'observed-targets', f'targetphot-{specprod}.fits')
        zcat_outfile = os.path.join(outdir, 'ancillary', f'targetphot-zcat-{specprod}.fits')

        allzcat = vstack(allzcat, metadata_conflicts='silent')
        allzcat.meta.pop('SURVEY')

        log.info(f'Writing {len(allzcat):,d} objects to {zcat_outfile}')
        allzcat.write(zcat_outfile, overwrite=True)

        alltargetphot = vstack(alltargetphot, metadata_conflicts='silent')
        try:
            assert(np.all(alltargetphot['TARGETID'] == allzcat['TARGETID']))
        except:
            errmsg = 'TARGETID mismatch!'
            log.critical(errmsg)
            raise IOError(errmsg)
        
        alltargetphot.meta.pop('SURVEY')
        log.info(f'Writing {len(alltargetphot):,d} objects to {targetphot_outfile}')
        alltargetphot.write(targetphot_outfile, overwrite=True)

        log.info(f'Total time for all surveys: {(time.time()-tall)/60.:.3f} min')

def targetphot_potential(specprod, reduxdir, outdir, comm=None, mp=1,
                         nside_targetphot=2, overwrite=False):
    """Gather targeting photometry for potential targets.

    """
    from desimodel.footprint import radec2pix
    
    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    RACOLUMN, DECCOLUMN = 'RA', 'DEC'
    
    if rank == 0:
        any_outfile = glob(os.path.join(outdir, 'potential-targets', f'targetphot-potential-*-{specprod}.fits'))
        if len(any_outfile) > 0 and not overwrite:
            log.warning('One or more targetphot output files exist; proceed with caution or use --overwrite.')

        # Assume that the nominal/parent zcat catalog has been previously generated.
        parent_zcatfile = os.path.join(outdir, 'ancillary', f'targetphot-zcat-{specprod}.fits')
        if not os.path.isfile(parent_zcatfile):
            errmsg = f'Targeting photometric catalog {parent_zcatfile} missing.'
            log.critical(errmsg)
            raise IOError(errmsg)

        parent_zcat = Table(fitsio.read(parent_zcatfile))
        log.info(f'Read {len(parent_zcat):,d} objects from {parent_zcatfile}.')

        _, uindx = np.unique(parent_zcat['TILEID'], return_index=True)
        tileids = parent_zcat['TILEID'][uindx]
        surveys = parent_zcat['SURVEY'][uindx]
        programs = parent_zcat['PROGRAM'][uindx]

        mpargs = [[tileid, survey, program] for tileid, survey, program in zip(tileids, surveys, programs)]
        if mp > 1:
            import multiprocessing
            with multiprocessing.Pool(mp) as P:
                zcat = P.map(_read_one_potential_targets, mpargs)
        else:
            zcat = [read_one_potential_targets(*mparg) for mparg in mpargs]
        zcat = vstack(zcat)

        # Do not select unique TARGETIDs because the same target can appear
        # in different surveys; we handle uniqueness below.
        log.info(f"Found {len(zcat):,d} objects from {len(set(zcat['TILEID'])):,d} unique tiles.")

        allzcat = []
        alltargetphot = []
        allminiphot = []

        tall = time.time()

    # Divide by survey.
    for survey in ALLSURVEYS:
        tone = time.time()

        # initialize
        zcat_survey = Table()
        pixels = np.array([])
        if survey == 'main':
            utiles = [np.array([])]   # list of tiles within each healpix
            groups = [[np.array([])]] # indices of tiles to process within each healpix
        else:
            utiles = np.array([])
            groups = [np.array([])]

        # Read all the ztile programs/catalogs in parallel on rank 0.
        if rank == 0:
            log.info(f'Working on survey {survey}')

            S = np.where(survey == zcat['SURVEY'])[0]
            if len(S) == 0:
                log.info(f'No redshift catalogs for survey:specprod {survey}:{specprod}')
                continue
            
            zcat_survey = zcat[S]
            allzcat.append(zcat_survey)

            utiles, npertile = np.unique(zcat_survey['TILEID'], return_counts=True)
            log.info(f'Found {len(zcat_survey):,d} TARGETIDs and {len(utiles):,d} unique tiles from survey: {survey}')

            # Split potential MAIN targets into nside=2 healpixels because there
            # are too many of them.
            if survey == 'main':
                pixels = radec2pix(nside_targetphot, zcat_survey['RA'], zcat_survey['DEC'])
                groups, utiles = [], []
                for ipix, pixel in enumerate(sorted(set(pixels))):
                    P = pixel == pixels
                    utiles_pixel, npertile = np.unique(zcat_survey[P]['TILEID'], return_counts=True)
                    utiles.append(utiles_pixel)
                    groups.append(weighted_partition(npertile, size))
            else:
                groups = weighted_partition(npertile, size)

        # broadcast the work to the other ranks
        if comm:
            zcat_survey = comm.bcast(zcat_survey, root=0)
            utiles = comm.bcast(utiles, root=0)
            pixels = comm.bcast(pixels, root=0)
            groups = comm.bcast(groups, root=0)

        # deal with MAIN separately
        if survey == 'main':
            if rank == 0:
                miniphot_survey = []
            
            for ipix, pixel in enumerate(sorted(set(pixels))):
                log.info(f'Rank {rank} started at {time.asctime()} on healpix {pixel} with {len(groups[ipix][rank])} tiles.')
            
                P = np.where(pixel == pixels)[0]
                zcat_pixel = zcat_survey[P]
            
                targetphot_pixel = []
                for tileid in utiles[ipix][groups[ipix][rank]]:
                    log.info(f'Rank {rank} is working on healpix {pixel} and tile {tileid}')
                    I = zcat_pixel['TILEID'] == tileid
                    _targetphot_onetile = targetphot_onetile(zcat_pixel[I], RACOLUMN, DECCOLUMN, survey)
                    assert(np.all(_targetphot_onetile['TARGETID'] == zcat_pixel[I]['TARGETID']))
                    targetphot_pixel.append(_targetphot_onetile)
                if len(targetphot_pixel) > 0:            
                    targetphot_pixel = vstack(targetphot_pixel)
    
                if comm:
                    targetphot_pixel = comm.gather(targetphot_pixel, root=0)

                # sort and write out on rank 0
                if rank == 0:
                    if len(targetphot_pixel) == 0:
                        errmsg = 'No targeting catalogs were generated!'
                        log.critical(errmsg)
                        raise ValueError(errmsg)
                    
                    targetphot_pixel = vstack(targetphot_pixel)
                
                    targetphot_pixel_outfile = os.path.join(outdir, 'potential-targets', f'targetphot-potential-nside{nside_targetphot}-hp{pixel:02d}-{survey}-{specprod}.fits')
                    targetphot_pixel, _, _ = write_targetphot(
                        targetphot_pixel, zcat_pixel, targetphot_outfile=targetphot_pixel_outfile,
                        zcat_outfile=None, miniphot_outfile=None, survey=survey, sort=True)
                    miniphot_survey.append(targetphot_pixel['TARGETID', 'TILEID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID'])

            if rank == 0:
                miniphot_survey = vstack(miniphot_survey)

                zcat_outfile = os.path.join(outdir, 'ancillary', f'targetphot-potential-zcat-{survey}-{specprod}.fits')
                miniphot_outfile = os.path.join(outdir, 'ancillary', f'targetphot-potential-miniphot-{survey}-{specprod}.fits')
                write_targetphot(miniphot_survey, zcat_survey, zcat_outfile=zcat_outfile,
                                 miniphot_outfile=miniphot_outfile, survey=survey, sort=True)

                
        else:
            log.info(f'Rank {rank} started at {time.asctime()} with {len(groups[rank])} tiles.')
            
            targetphot_survey = []
            for tileid in utiles[groups[rank]]:
                log.info(f'Rank {rank} is working on tile {tileid}')
                I = zcat_survey['TILEID'] == tileid
                _targetphot_onetile = targetphot_onetile(zcat_survey[I], RACOLUMN, DECCOLUMN, survey)
                assert(np.all(_targetphot_onetile['TARGETID'] == zcat_survey[I]['TARGETID']))
                targetphot_survey.append(_targetphot_onetile)
            if len(targetphot_survey) > 0:            
                targetphot_survey = vstack(targetphot_survey)

            if comm:
                targetphot_survey = comm.gather(targetphot_survey, root=0)

            # sort and write out on rank 0
            if rank == 0:
                if len(targetphot_survey) == 0:
                    errmsg = 'No targeting catalogs were generated!'
                    log.critical(errmsg)
                    raise ValueError(errmsg)
                targetphot_survey = vstack(targetphot_survey)

                targetphot_outfile = os.path.join(outdir, 'potential-targets', f'targetphot-potential-{survey}-{specprod}.fits')
                zcat_outfile = os.path.join(outdir, 'ancillary', f'targetphot-potential-zcat-{survey}-{specprod}.fits')
                #miniphot_outfile = os.path.join(outdir, 'ancillary', f'targetphot-potential-miniphot-{survey}-{specprod}.fits')
                miniphot_outfile = None

                targetphot_survey, zcat_survey, miniphot_survey = write_targetphot(
                    targetphot_survey, zcat_survey, targetphot_outfile=targetphot_outfile,
                    zcat_outfile=zcat_outfile, miniphot_outfile=miniphot_outfile, survey=survey,
                    sort=True)
                assert(np.all(targetphot_survey['TARGETID'] == zcat_survey['TARGETID']))            
    
                alltargetphot.append(targetphot_survey)
                #allminiphot.append(miniphot_survey)
                del targetphot_survey, zcat_survey, miniphot_survey
    
            log.info(f'Total time for survey {survey}: {(time.time()-tone)/60.:.3f} min')            

        log.info(f'Rank {rank} is all done!')

    # write out the final merged redshift catalog.
    if rank == 0:
        if len(allzcat) > 0:
            zcat_outfile = os.path.join(outdir, 'ancillary', f'targetphot-potential-zcat-{specprod}.fits')

            allzcat = vstack(allzcat, metadata_conflicts='silent')
            log.info(f'Writing {len(allzcat):,d} objects to {zcat_outfile}')
            allzcat.write(zcat_outfile, overwrite=True)

        log.info(f'Total time for all surveys: {(time.time()-tall)/60.:.3f} min')


def main():
    """Main wrapper.

    """
    import argparse
    from desispec.io import specprod_root
    
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--specprod', type=str, required=True, help='Spectroscopic production; output file prefix.')
    parser.add_argument('--reduxdir', type=str, help='spectro redux base dir overrides $DESI_SPECTRO_REDUX/$SPECPROD')
    parser.add_argument('--outdir', default='$PSCRATCH/lsdr9', type=str, help='Base output data directory.')

    parser.add_argument('--targetphot', action='store_true', help='Build the observed targets targeting catalogs.')
    parser.add_argument('--tractorphot', action='store_true', help='Build the observed targets tractor catalogs.')
    parser.add_argument('--targetphot-potential', action='store_true', help='Build the potential targets targeting catalogs.')
    parser.add_argument('--tractorphot-potential', action='store_true', help='Build the potential targets tractor catalogs.')

    parser.add_argument('--validate-targetphot', action='store_true', help='Validate the targetphot catalogs.')
    parser.add_argument('--validate-tractorphot', action='store_true', help='Validate the photo-tractor catalogs.')
    
    parser.add_argument('--mp', type=int, default=1, help='Number of multiprocessing processes per MPI rank or node.')
    parser.add_argument('--nside-tractorphot', type=int, default=4, help='healpix nside for tractorphot catalogs.')
    parser.add_argument('--nside-targetphot', type=int, default=2, help='healpix nside for targetphot-main catalogs.')

    parser.add_argument('--plan', action='store_true', help='Plan how many nodes to use and how to distribute the targets.')
    parser.add_argument('--nompi', action='store_true', help='Do not use MPI parallelism.')
    parser.add_argument('--dry-run', action='store_true', help='Generate but do not run commands.')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite any existing output files.')
    
    args = parser.parse_args()
    log = get_logger()    

    args.outdir = os.path.expandvars(args.outdir)

    if args.reduxdir is None:
        args.reduxdir = specprod_root()
        
    for subdir in ['observed-targets', 'potential-targets']:
        if not os.path.isdir(os.path.join(args.outdir, subdir)):
            os.makedirs(os.path.join(args.outdir, subdir), exist_ok=True)
        if not os.path.isdir(os.path.join(args.outdir, subdir, 'tractorphot')):
            os.makedirs(os.path.join(args.outdir, subdir, 'tractorphot'), exist_ok=True)
    if not os.path.isdir(os.path.join(args.outdir, 'ancillary')):
        os.makedirs(os.path.join(args.outdir, 'ancillary'), exist_ok=True)

    if args.nompi:
        comm = None
    else:
        try:
            from mpi4py import MPI
            comm = MPI.COMM_WORLD
        except ImportError:
            comm = None

    # https://docs.nersc.gov/development/languages/python/parallel-python/#use-the-spawn-start-method
    if args.mp > 1 and 'NERSC_HOST' in os.environ:
        import multiprocessing
        multiprocessing.set_start_method('spawn')

    if args.targetphot:
        targetphot_observed(args.specprod, args.reduxdir, args.outdir, comm=comm,
                            mp=args.mp, overwrite=args.overwrite)
        
    if args.targetphot_potential:
        targetphot_potential(args.specprod, args.reduxdir, args.outdir, comm=comm,
                             mp=args.mp, nside_targetphot=args.nside_targetphot,
                             overwrite=args.overwrite)
                    
    #if args.tractorphot:
    #    if args.potential:
    #        tractorphot_potential(args, comm=comm)
    #    else:
    #        tractorphot(args, comm=comm)

if __name__ == '__main__':
    main()
